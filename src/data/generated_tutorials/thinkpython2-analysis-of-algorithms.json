{
  "title": "Analysis of Algorithms",
  "slug": "thinkpython2-analysis-of-algorithms",
  "description": "Module from thinkpython2.pdf: Analysis of Algorithms",
  "tags": [
    "thinkpython2",
    "Elite"
  ],
  "content": "# Analysis of Algorithms\n\n\n\nThis appendix is an edited excerpt from Think Complexity , by Allen B. Downey, \nalso published by O\u2019Reilly Media (2012). When you are done with this book, \nyou might want to move on to that one. \n\nAnalysis of algorithms is a branch of computer science that studies the performance of \nalgorithms, especially their run time and space requirements. See http://en.wikipedia. \norg/wiki/Analysis_of_algorithms . \n\nThe practical goal of algorithm analysis is to predict the performance of different algo- \nrithms in order to guide design decisions. \n\nDuring the 2008 United States Presidential Campaign, candidate Barack Obama was asked \nto perform an impromptu analysis when he visited Google. Chief executive Eric Schmidt \njokingly asked him for \u201cthe most ef\ufb01cient way to sort a million 32-bit integers.\u201d Obama \nhad apparently been tipped off, because he quickly replied, \u201cI think the bubble sort would \nbe the wrong way to go.\u201d See http://www.youtube.com/watch?v=k4RRi_ntQc8 . \n\nThis is true: bubble sort is conceptually simple but slow for large datasets. \nThe an- \nswer Schmidt was probably looking for is \u201cradix sort\u201d ( http://en.wikipedia.org/wiki/ \nRadix_sort ) 1 . \n\nThe goal of algorithm analysis is to make meaningful comparisons between algorithms, \nbut there are some problems: \n\n\u2022 The relative performance of the algorithms might depend on characteristics of the \nhardware, so one algorithm might be faster on Machine A, another on Machine B. \nThe general solution to this problem is to specify a machine model and analyze the \nnumber of steps, or operations, an algorithm requires under a given model. \n\n\u2022 Relative performance might depend on the details of the dataset. For example, some \nsorting algorithms run faster if the data are already partially sorted; other algorithms \n\n1 But if you get a question like this in an interview, I think a better answer is, \u201cThe fastest way to sort a million \nintegers is to use whatever sort function is provided by the language I\u2019m using. Its performance is good enough \nfor the vast majority of applications, but if it turned out that my application was too slow, I would use a pro\ufb01ler \nto see where the time was being spent. If it looked like a faster sort algorithm would have a signi\ufb01cant effect on \nperformance, then I would look around for a good implementation of radix sort.\u201d \n\n202 \nAppendix B. Analysis of Algorithms \n\nrun slower in this case. A common way to avoid this problem is to analyze the worst \ncase scenario. It is sometimes useful to analyze average case performance, but that\u2019s \nusually harder, and it might not be obvious what set of cases to average over. \n\n\u2022 Relative performance also depends on the size of the problem. A sorting algorithm \nthat is fast for small lists might be slow for long lists. The usual solution to this \nproblem is to express run time (or number of operations) as a function of problem \nsize, and group functions into categories depending on how quickly they grow as \nproblem size increases. \n\nThe good thing about this kind of comparison is that it lends itself to simple classi\ufb01cation \nof algorithms. For example, if I know that the run time of Algorithm A tends to be pro- \nportional to the size of the input, n , and Algorithm B tends to be proportional to n 2 , then I \nexpect A to be faster than B, at least for large values of n . \n\nThis kind of analysis comes with some caveats, but we\u2019ll get to that later. \n\nB.1 \nOrder of growth \n\nSuppose you have analyzed two algorithms and expressed their run times in terms of the \nsize of the input: Algorithm A takes 100 n + 1 steps to solve a problem with size n ; Algo- \nrithm B takes n 2 + n + 1 steps. \n\nThe following table shows the run time of these algorithms for different problem sizes: \n\nInput \nRun time of \nRun time of \nsize \nAlgorithm A \nAlgorithm B \n10 \n1 001 \n111 \n100 \n10 001 \n10 101 \n1 000 \n100 001 \n1 001 001 \n10 000 \n1 000 001 \n100 010 001 \n\nAt n = 10, Algorithm A looks pretty bad; it takes almost 10 times longer than Algorithm \nB. But for n = 100 they are about the same, and for larger values A is much better. \n\nThe fundamental reason is that for large values of n , any function that contains an n 2 term \nwill grow faster than a function whose leading term is n . The leading term is the term with \nthe highest exponent. \n\nFor Algorithm A, the leading term has a large coef\ufb01cient, 100, which is why B does better \nthan A for small n . But regardless of the coef\ufb01cients, there will always be some value of n \nwhere an 2 > bn , for any values of a and b . \n\nThe same argument applies to the non-leading terms. Even if the run time of Algorithm A \nwere n + 1000000, it would still be better than Algorithm B for suf\ufb01ciently large n . \n\nIn general, we expect an algorithm with a smaller leading term to be a better algorithm for \nlarge problems, but for smaller problems, there may be a crossover point where another \nalgorithm is better. The location of the crossover point depends on the details of the algo- \nrithms, the inputs, and the hardware, so it is usually ignored for purposes of algorithmic \nanalysis. But that doesn\u2019t mean you can forget about it. \n\nB.1. Order of growth \n203 \n\nIf two algorithms have the same leading order term, it is hard to say which is better; again, \nthe answer depends on the details. So for algorithmic analysis, functions with the same \nleading term are considered equivalent, even if they have different coef\ufb01cients. \n\nAn order of growth is a set of functions whose growth behavior is considered equivalent. \nFor example, 2 n , 100 n and n + 1 belong to the same order of growth, which is written O ( n ) \nin Big-Oh notation and often called linear because every function in the set grows linearly \nwith n . \n\nAll functions with the leading term n 2 belong to O ( n 2 ) ; they are called quadratic . \n\nThe following table shows some of the orders of growth that appear most commonly in \nalgorithmic analysis, in increasing order of badness. \n\nOrder of \nName \ngrowth \n\nO ( 1 ) \nconstant \nO ( log b n ) \nlogarithmic (for any b ) \nO ( n ) \nlinear \nO ( n log b n ) \nlinearithmic \nO ( n 2 ) \nquadratic \nO ( n 3 ) \ncubic \nO ( c n ) \nexponential (for any c ) \n\nFor the logarithmic terms, the base of the logarithm doesn\u2019t matter; changing bases is the \nequivalent of multiplying by a constant, which doesn\u2019t change the order of growth. Sim- \nilarly, all exponential functions belong to the same order of growth regardless of the base \nof the exponent. Exponential functions grow very quickly, so exponential algorithms are \nonly useful for small problems. \nExercise B.1. Read the Wikipedia page on Big-Oh notation at http: // en. wikipedia. org/ \nwiki/ Big_ O_ notation and answer the following questions: \n\n1. What is the order of growth of n 3 + n 2 ? What about 1000000 n 3 + n 2 ? What about n 3 + \n1000000 n 2 ? \n\n2. What is the order of growth of ( n 2 + n ) \u00b7 ( n + 1 ) ? Before you start multiplying, remember \nthat you only need the leading term. \n\n3. If f is in O ( g ) , for some unspeci\ufb01ed function g, what can we say about a f + b, where a and \nb are constants? \n\n4. If f 1 and f 2 are in O ( g ) , what can we say about f 1 + f 2 ? \n\n5. If f 1 is in O ( g ) and f 2 is in O ( h ) , what can we say about f 1 + f 2 ? \n\n6. If f 1 is in O ( g ) and f 2 is O ( h ) , what can we say about f 1 \u00b7 f 2 ? \n\nProgrammers who care about performance often \ufb01nd this kind of analysis hard to swal- \nlow. They have a point: sometimes the coef\ufb01cients and the non-leading terms make a real \ndifference. Sometimes the details of the hardware, the programming language, and the \ncharacteristics of the input make a big difference. And for small problems, order of growth \nis irrelevant. \n\nBut if you keep those caveats in mind, algorithmic analysis is a useful tool. At least for \nlarge problems, the \u201cbetter\u201d algorithm is usually better, and sometimes it is much better. \nThe difference between two algorithms with the same order of growth is usually a constant \nfactor, but the difference between a good algorithm and a bad algorithm is unbounded! \n\n204 \nAppendix B. Analysis of Algorithms \n\nB.2 \nAnalysis of basic Python operations \n\nIn Python, most arithmetic operations are constant time; multiplication usually takes \nlonger than addition and subtraction, and division takes even longer, but these run times \ndon\u2019t depend on the magnitude of the operands. Very large integers are an exception; in \nthat case the run time increases with the number of digits. \n\nIndexing operations\u2014reading or writing elements in a sequence or dictionary\u2014are also \nconstant time, regardless of the size of the data structure. \n\nA for loop that traverses a sequence or dictionary is usually linear, as long as all of the \noperations in the body of the loop are constant time. For example, adding up the elements \nof a list is linear: \n\ntotal = 0 \nfor x in t: \ntotal += x \n\nThe built-in function sum is also linear because it does the same thing, but it tends to be \nfaster because it is a more ef\ufb01cient implementation; in the language of algorithmic analysis, \nit has a smaller leading coef\ufb01cient. \n\nAs a rule of thumb, if the body of a loop is in O ( n a ) then the whole loop is in O ( n a + 1 ) . The \nexception is if you can show that the loop exits after a constant number of iterations. If a \nloop runs k times regardless of n , then the loop is in O ( n a ) , even for large k . \n\nMultiplying by k doesn\u2019t change the order of growth, but neither does dividing. So if the \nbody of a loop is in O ( n a ) and it runs n / k times, the loop is in O ( n a + 1 ) , even for large k . \n\nMost string and tuple operations are linear, except indexing and len , which are constant \ntime. The built-in functions min and max are linear. The run-time of a slice operation is \nproportional to the length of the output, but independent of the size of the input. \n\nString concatenation is linear; the run time depends on the sum of the lengths of the \noperands. \n\nAll string methods are linear, but if the lengths of the strings are bounded by a constant\u2014 \nfor example, operations on single characters\u2014they are considered constant time. \nThe \nstring method join is linear; the run time depends on the total length of the strings. \n\nMost list methods are linear, but there are some exceptions: \n\n\u2022 Adding an element to the end of a list is constant time on average; when it runs \nout of room it occasionally gets copied to a bigger location, but the total time for n \noperations is O ( n ) , so the average time for each operation is O ( 1 ) . \n\n\u2022 Removing an element from the end of a list is constant time. \n\n\u2022 Sorting is O ( n log n ) . \n\nMost dictionary operations and methods are constant time, but there are some exceptions: \n\n\u2022 The run time of update is proportional to the size of the dictionary passed as a pa- \nrameter, not the dictionary being updated. \n\n\u2022 keys , values and items are constant time because they return iterators. But if you \nloop through the iterators, the loop will be linear. \n\nB.3. Analysis of search algorithms \n205 \n\nThe performance of dictionaries is one of the minor miracles of computer science. We will \nsee how they work in Section B.4. \nExercise B.2. Read the Wikipedia page on sorting algorithms at http: // en. wikipedia. org/ \nwiki/ Sorting_ algorithm and answer the following questions: \n\n1. What is a \u201ccomparison sort?\u201d What is the best worst-case order of growth for a comparison \nsort? What is the best worst-case order of growth for any sort algorithm? \n\n2. What is the order of growth of bubble sort, and why does Barack Obama think it is \u201cthe wrong \nway to go?\u201d \n\n3. What is the order of growth of radix sort? What preconditions do we need to use it? \n\n4. What is a stable sort and why might it matter in practice? \n\n5. What is the worst sorting algorithm (that has a name)? \n\n6. What sort algorithm does the C library use? What sort algorithm does Python use? Are these \nalgorithms stable? You might have to Google around to \ufb01nd these answers. \n\n7. Many of the non-comparison sorts are linear, so why does Python use an O ( n log n ) compar- \nison sort? \n\nB.3 \nAnalysis of search algorithms \n\nA search is an algorithm that takes a collection and a target item and determines whether \nthe target is in the collection, often returning the index of the target. \n\nThe simplest search algorithm is a \u201clinear search\u201d, which traverses the items of the collec- \ntion in order, stopping if it \ufb01nds the target. In the worst case it has to traverse the entire \ncollection, so the run time is linear. \n\nThe in operator for sequences uses a linear search; so do string methods like find and \ncount . \n\nIf the elements of the sequence are in order, you can use a bisection search , which is \nO ( log n ) . Bisection search is similar to the algorithm you might use to look a word up \nin a dictionary (a paper dictionary, not the data structure). Instead of starting at the be- \nginning and checking each item in order, you start with the item in the middle and check \nwhether the word you are looking for comes before or after. If it comes before, then you \nsearch the \ufb01rst half of the sequence. Otherwise you search the second half. Either way, you \ncut the number of remaining items in half. \n\nIf the sequence has 1,000,000 items, it will take about 20 steps to \ufb01nd the word or conclude \nthat it\u2019s not there. So that\u2019s about 50,000 times faster than a linear search. \n\nBisection search can be much faster than linear search, but it requires the sequence to be in \norder, which might require extra work. \n\nThere is another data structure, called a hashtable that is even faster\u2014it can do a search \nin constant time\u2014and it doesn\u2019t require the items to be sorted. Python dictionaries are \nimplemented using hashtables, which is why most dictionary operations, including the in \noperator, are constant time. \n\n206 \nAppendix B. Analysis of Algorithms \n\nB.4 \nHashtables \n\nTo explain how hashtables work and why their performance is so good, I start with a simple \nimplementation of a map and gradually improve it until it\u2019s a hashtable. \n\nI use Python to demonstrate these implementations, but in real life you wouldn\u2019t write \ncode like this in Python; you would just use a dictionary! So for the rest of this chapter, you \nhave to imagine that dictionaries don\u2019t exist and you want to implement a data structure \nthat maps from keys to values. The operations you have to implement are: \n\nadd(k, v) : Add a new item that maps from key k to value v . With a Python dictionary, d , \nthis operation is written d[k] = v . \n\nget(k) : Look up and return the value that corresponds to key k . With a Python dictionary, \nd , this operation is written d[k] or d.get(k) . \n\nFor now, I assume that each key only appears once. The simplest implementation of this \ninterface uses a list of tuples, where each tuple is a key-value pair. \n\nclass LinearMap: \n\ndef __init__(self): \nself.items = [] \n\ndef add(self, k, v): \nself.items.append((k, v)) \n\ndef get(self, k): \nfor key, val in self.items: \nif key == k: \nreturn val \nraise KeyError \n\nadd appends a key-value tuple to the list of items, which takes constant time. \n\nget uses a for loop to search the list: if it \ufb01nds the target key it returns the corresponding \nvalue; otherwise it raises a KeyError . So get is linear. \n\nAn alternative is to keep the list sorted by key. Then get could use a bisection search, which \nis O ( log n ) . But inserting a new item in the middle of a list is linear, so this might not be the \nbest option. There are other data structures that can implement add and get in log time, \nbut that\u2019s still not as good as constant time, so let\u2019s move on. \n\nOne way to improve LinearMap is to break the list of key-value pairs into smaller lists. \nHere\u2019s an implementation called BetterMap , which is a list of 100 LinearMaps. As we\u2019ll \nsee in a second, the order of growth for get is still linear, but BetterMap is a step on the \npath toward hashtables: \n\nclass BetterMap: \n\ndef __init__(self, n=100): \nself.maps = [] \nfor i in range(n): \nself.maps.append(LinearMap()) \n\nB.4. Hashtables \n207 \n\ndef find_map(self, k): \nindex = hash(k) % len(self.maps) \nreturn self.maps[index] \n\ndef add(self, k, v): \nm = self.find_map(k) \nm.add(k, v) \n\ndef get(self, k): \nm = self.find_map(k) \nreturn m.get(k) \n\n__init__ makes a list of n LinearMap s. \n\nfind_map is used by add and get to \ufb01gure out which map to put the new item in, or which \nmap to search. \n\nfind_map uses the built-in function hash , which takes almost any Python object and returns \nan integer. A limitation of this implementation is that it only works with hashable keys. \nMutable types like lists and dictionaries are unhashable. \n\nHashable objects that are considered equivalent return the same hash value, but the con- \nverse is not necessarily true: two objects with different values can return the same hash \nvalue. \n\nfind_map uses the modulus operator to wrap the hash values into the range from 0 to \nlen(self.maps) , so the result is a legal index into the list. Of course, this means that many \ndifferent hash values will wrap onto the same index. But if the hash function spreads things \nout pretty evenly (which is what hash functions are designed to do), then we expect n /100 \nitems per LinearMap. \n\nSince the run time of LinearMap.get is proportional to the number of items, we expect \nBetterMap to be about 100 times faster than LinearMap. The order of growth is still linear, \nbut the leading coef\ufb01cient is smaller. That\u2019s nice, but still not as good as a hashtable. \n\nHere (\ufb01nally) is the crucial idea that makes hashtables fast: if you can keep the maximum \nlength of the LinearMaps bounded, LinearMap.get is constant time. All you have to do is \nkeep track of the number of items and when the number of items per LinearMap exceeds \na threshold, resize the hashtable by adding more LinearMaps. \n\nHere is an implementation of a hashtable: \n\nclass HashMap: \n\ndef __init__(self): \nself.maps = BetterMap(2) \nself.num = 0 \n\ndef get(self, k): \nreturn self.maps.get(k) \n\ndef add(self, k, v): \nif self.num == len(self.maps.maps): \n\n208 \nAppendix B. Analysis of Algorithms \n\nself.resize() \n\nself.maps.add(k, v) \nself.num += 1 \n\ndef resize(self): \nnew_maps = BetterMap(self.num * 2) \n\nfor m in self.maps.maps: \nfor k, v in m.items: \nnew_maps.add(k, v) \n\nself.maps = new_maps \n\n__init__ creates a BetterMap and initializes num , which keeps track of the number of items. \n\nget just dispatches to BetterMap . The real work happens in add , which checks the number \nof items and the size of the BetterMap : if they are equal, the average number of items per \nLinearMap is 1, so it calls resize . \n\nresize makes a new BetterMap , twice as big as the previous one, and then \u201crehashes\u201d the \nitems from the old map to the new. \n\nRehashing is necessary because changing the number of LinearMaps changes the denomi- \nnator of the modulus operator in find_map . That means that some objects that used to hash \ninto the same LinearMap will get split up (which is what we wanted, right?). \n\nRehashing is linear, so resize is linear, which might seem bad, since I promised that add \nwould be constant time. But remember that we don\u2019t have to resize every time, so add is \nusually constant time and only occasionally linear. The total amount of work to run add n \ntimes is proportional to n , so the average time of each add is constant time! \n\nTo see how this works, think about starting with an empty HashTable and adding a se- \nquence of items. We start with 2 LinearMaps, so the \ufb01rst 2 adds are fast (no resizing re- \nquired). Let\u2019s say that they take one unit of work each. The next add requires a resize, so \nwe have to rehash the \ufb01rst two items (let\u2019s call that 2 more units of work) and then add the \nthird item (one more unit). Adding the next item costs 1 unit, so the total so far is 6 units \nof work for 4 items. \n\nThe next add costs 5 units, but the next three are only one unit each, so the total is 14 units \nfor the \ufb01rst 8 adds. \n\nThe next add costs 9 units, but then we can add 7 more before the next resize, so the total is \n30 units for the \ufb01rst 16 adds. \n\nAfter 32 adds, the total cost is 62 units, and I hope you are starting to see a pattern. After n \nadds, where n is a power of two, the total cost is 2 n \u2212 2 units, so the average work per add \nis a little less than 2 units. When n is a power of two, that\u2019s the best case; for other values of \nn the average work is a little higher, but that\u2019s not important. The important thing is that it \nis O ( 1 ) . \n\nFigure B.1 shows how this works graphically. Each block represents a unit of work. The \ncolumns show the total work for each add in order from left to right: the \ufb01rst two adds cost \n1 unit each, the third costs 3 units, etc. \n\nB.5. Glossary \n209 \n\nFigure B.1: The cost of a hashtable add. \n\nThe extra work of rehashing appears as a sequence of increasingly tall towers with increas- \ning space between them. Now if you knock over the towers, spreading the cost of resizing \nover all adds, you can see graphically that the total cost after n adds is 2 n \u2212 2. \n\nAn important feature of this algorithm is that when we resize the HashTable it grows \ngeometrically; that is, we multiply the size by a constant. \nIf you increase the size \narithmetically\u2014adding a \ufb01xed number each time\u2014the average time per add is linear. \n\nYou can download my implementation of HashMap from https://thinkpython.com/ \ncode/Map.py , but remember that there is no reason to use it; if you want a map, just use a \nPython dictionary. \n\nB.5 \nGlossary \n\nanalysis of algorithms: A way to compare algorithms in terms of their run time and/or \nspace requirements. \n\nmachine model: A simpli\ufb01ed representation of a computer used to describe algorithms. \n\nworst case: The input that makes a given algorithm run slowest (or require the most \nspace). \n\nleading term: In a polynomial, the term with the highest exponent. \n\ncrossover point: The problem size where two algorithms require the same run time or \nspace. \n\norder of growth: A set of functions that all grow in a way considered equivalent for pur- \nposes of analysis of algorithms. For example, all functions that grow linearly belong \nto the same order of growth. \n\nBig-Oh notation: Notation for representing an order of growth; for example, O ( n ) repre- \nsents the set of functions that grow linearly. \n\nlinear: An algorithm whose run time is proportional to problem size, at least for large \nproblem sizes. \n\nquadratic: An algorithm whose run time is proportional to n 2 , where n is a measure of \nproblem size. \n\nsearch: The problem of locating an element of a collection (like a list or dictionary) or \ndetermining that it is not present. \n\n210 \nAppendix B. Analysis of Algorithms \n\nhashtable: A data structure that represents a collection of key-value pairs and performs \nsearch in constant time. \n\n",
  "createdAt": "2026-02-17"
}