{
  "title": "5",
  "slug": "iot2-5",
  "description": "Module from iot2.pdf: 5",
  "tags": [
    "iot2",
    "Elite"
  ],
  "content": "# 5\n\nData Acquiring, Organising, \n\nProcessing and Analytics \n\nLearning Objectives \n\nLO 5.1 Apply the data-acquiring and data-storage functions for IoT/M2M devices data and messages LO 5.2 Classify ways of organising data LO 5.3 Summarise the transactions on stored data, functions for business processes and business intelligence, and the concepts of IoT applications\u2014integration and services architecture LO 5.4 Identify the functions and usage of data analytics and data visualisations for IoT applications and business processes LO 5.5 Explain knowledge discovery, knowledge management and knowledge-management reference architecture \n\nRecall from Previous Chapters \n\nLessons for IoT/M2M architectural layers and functions learnt in previous chapters, are\u2014devices communicate, first over a local network or WPAN and then send the physical layer data to data- adaptation and gateway layer. The gateway connects to the Internet, and communicates the data packets. The packets communicate over the Internet through a set of routers. Application and application-support layers use the acquired and collected data for IoT applications. Applications can also control and monitor the functions of devices. The application messages, commands and data communicates to devices through the gateway using the Internet. \n\n156 Internet of Things: Architecture and Design Principles \n\n5.1 INTRODUCTION \n\nHaving learnt about devices, devices-network data, messages and packet communication to the Internet, let us understand the functions required for applications, services and business processes at application-support and application layers. These functions are data acquiring, data storage, data transactions, analytics, results visualisations, IoT applications integration, services, processes, intelligence, knowledge discovery and knowledge management. \n\nLet us first discuss the following terms and their meanings used in IoT application layers. \n\nApplication refers to application software or a collection of software components. An application enables a user to perform a group of coordinated activities, functions and tasks. Streetlights control and monitoring is an example of an application. Software for tracking and inventory control are other examples of applications. Tracking applications use tags and locations data of the RFIDs. \n\nAn application enables a user to withdraw cash using an Automatic Teller Machine (ATM). An umbrella sending warning messages for weather (Example 1.1), a waste container management, health monitoring, traffic lights control, synchronisation and monitoring are other examples of IoT applications. \n\nService denotes a mechanism, which enables the provisioning of access to one or more capabilities. An interface for the service provides the access to capabilities. The access to each capability is consistent with constraints and policies, which a service-description specifies. Examples of service capabilities are automotive maintenance service capabilities or service capabilities for the Automatic Chocolate Vending Machines (ACVMs) for timely filling of chocolates into the machines. \n\nService consists of a set of related software components and their functionalities. The set is reused for one or more purposes. Usage of the set is consistent with the controls, constraints and policies which are specified in the service description for each service. A service also associates a Service Level Agreement (SLA). \n\nA service consists of a collection of self-contained, distinct and reusable components. It provides logically grouped and encapsulated functionalities. Traffic lights synchronising service, automobile maintenance service, devices location, detection and tracking service, home security-breach detection and management service, waste containers substitution service, and health-alerts service are the examples of IoT services. \n\nService Oriented Architecture (SOA) is a software architecture model, which consists of services, messages, operations and processes. SOA components are distributed over a network or the Internet in a high-level business entity. New business applications and applications integration architecture in an enterprise can be developed using an SOA. \n\nData Acquiring, Organising, Processing and Analytics 157 \n\nMessage means a communicating entity or object. \n\nOperation means action or set of actions. For example, actions during a bank transaction. \n\nTransaction (trans + action) refers to two inter-related sets of operations or actions or instructions. For example, a transaction may be access to sales data to select and get the annual sales in a specific year in return. One operation is access to sales data and other is annual sales in return. Another example of a transaction is a query transaction with a Database Management System (DBMS). \n\nQuery is a command for getting select values from a database which in return transfer the answer to the query after its processing. A query example is command to ACVMs database for providing sales data of ACVMs on Sundays near city gardens in a specific festival period in a year. Another example is query to service center database for providing the list of automobile components needing replacement that have completed expected service-life in a specific vehicle. \n\nQuery Processing is a group of structured activities undertaken to get the results from a data store as per the query. \n\nKey Value Pair (KVP) refers to a set of two linked entities, one is the key, which is a unique identifier for a linked entity and the other is the value, which is either the entity that is identified or a pointer to the location of that entity. A KVP example is birthday-date pair. KVP is birthday: July 17, 2000. Birthday is the key for a table and date July 17, 2000 is the value. KVP applications create the look-up tables, hash tables and the network or device configuration files. \n\nHash Table (also called hash map) refers to a data structure which maps the KVPs and is used to implement an associative array (for example array of KVPs). A hash table may use an index (key) which is computed using a hash function and key maps to the value. Index is used to get or point to the desired value. \n\nBigtable maps two arbitrary string values into an associated arbitrary byte array. One is used as row key and the other as column key. Time stamp associates in three-dimensional mapping. Mapping is unlike a relational database but can be considered as a sparse, distributed multi-dimensional sorted map. The table can scale up to 100s to 1000s of distributed computing nodes with ease of adding more nodes. \n\nBusiness Transaction (BT) in database theory, refers to a (business) process that requests information from or that changes the data in a database.One operation in a BT is a command \u2018connect\u2019 that connects a DBMS and database, which in turn also connects with the DBMS. Similarly, BTs are processes using commands \u2018insert\u2019, \u2018delete\u2019, \u2018append\u2019, and \u2018modify\u2019. \n\nProcess means a composition of a group of structured activities or tasks that lead to a particular goal (or that interact to achieve a result). For example, streetlights control \n\n158 Internet of Things: Architecture and Design Principles \n\nprocess of the purchase process for an airline ticket. A process specifies activities with relevance rules based on data in the process. \n\nProcess Matrix is a multi-element entity, each element of which relates a set of data or inputs to an activity (or subset of activities). \n\nBusiness Process (BP) is an activity or series of activities or a collection of inter-related structured activities, tasks or processes. A BP serves a particular goal or specific result or service or product. The BP is a representation or process matrix or flowchart of a sequence of activities with interleaving decision points; interleaving means putting in between. Decision point means an instance in a series of activities when decisions are taken for further activities. \n\nA web1 definition states, \u201ca BP is a specific event in a chain of structured business activities that typically change the state of data and/or a product and generate some type of output\u201d. Examples of BPs include finding the annual sales growth and managing the supplies. Another definition2 of BP is that \u201cbusiness process is an activity or set of activities that will accomplish a specific organizational goal\u201d. One more definition3 of BP states, \u201cBP is a series of logically related activities or tasks (such as planning, production, or sales) performed together to produce a defined set of results.\u201d \n\nBusiness Intelligence (BI) is a process which enables a business service to extract new facts and knowledge, and then undertake better decisions. These new facts and knowledge follow from earlier results of data processing, aggregation and analysis of these results. \n\nExample 5.1 clarifies the meanings of application, service, SOA, BP and BI for an Internet connected chain of ACVMs to enable the understanding of what these terms do mean. \n\nExample 5.1 \n\nProblem Assume a connected chain of ACVMs spread all over a city. Each ACVM delivers chocolate at each instance as per the user\u2019s choice among one of the 5 flavours, viz. FL1, FL2, FL3, FL4 and FL5. A chosen chocolate delivers on insertion of coins of appropriate amount as per the cost. A display unit displays user interfaces and takes part in user interaction when a child wishes to buy and get the chocolate of her/his choice. The ACVM also displays advertisements for chocolates, news, weather reports and events in the city whenever not in use. Each ACVM connects to the Internet for its management and services. What do application, service, SOA, business process and business intelligence mean in the Internet of ACVMs? \n\nSolution \n\nApplication refers to software that provides for the management of ACVMs. When a new ACVM installs, then the Manager updates the database of machine IDs. The manager programs the ACVMs for the services and also does the diagnosis of each ACVM at regular intervals. This helps in locating the improper functioning \n\n1 http://www.webopedia.com/TERM/B/business_process.html 2 http://searchcio.techtarget.com/definition/business-process 3 http://www.businessdictionary.com/definition/business-process.html \n\nData Acquiring, Organising, Processing and Analytics 159 \n\nACVMs. When the Manager finds a specific ACVM faulty, he/she initiates the required steps. For example, sending SMS to owner; informing the ACVM maintenance service which sends a mechanic; informing the chocolate-filling service, and so on. Service refers to software, which initiates filling of the ACVMs with chocolates of distinct flavours. Each service has a service description for usage by a Manager application. Service includes a Service Level Agreement (SLA) with the Manager. Service initiates actions on message from the Manager. For example, Manager periodically delivers the messages for events and alerts for the sell up to the specified threshold levels at each ACVM for each flavour. This enables service to optimally plan the filling of the ACVMs. \n\nACVMs manager can deploy number of services, like collection of coins service. Each service has the service description (desc) and SLA. Service software distinguishes itself from an application due to its usages, not only specific to one application but can also be to many applications. A service selection uses desc and enters into an SLA between the service and application. \n\nSOA is architecture models all the ACVM services, their interactions and the initiation of each service on a distinct message from an application, service or process to another. SOA describes the services, other components, their interactions, the sequences of messages, operations and processes. \n\nProcess refers to series of activities such as acquiring data from the ACVMs for the counts of each flavour sold during programmed intervals, analysing the acquired data, initiating messages for Fill service for each flavour through Fill process. Another series of activities such as acquiring data for the amount collected at each of the ACVMs at programmed intervals, analysing the acquired data; and initiating Collect service for each flavour through Collect process. \n\nBusiness process refers to series of related processes, such as Fill, Collect and Display at appropriate intervals is a business process in ACVMs chain. Business intelligence is a process which enables the ACVM business service to extract new facts and knowledge. This enables the undertaking of better decisions for new option(s) to maximise profits. For example, dropping or reduced loading of the ACVMs with specific flavour(s) in select regions or introducing new alternative flavours or advertising for the flavours getting less favour from the children, or relocating ACVMs in specific areas or adopting enterprise strategy for intimacy with children such as free chocolate on birthdays or awarding loyalty points. \n\nNew facts and knowledge follows from the earlier acquired results of data processing and aggregation, and the region-wise, segmentation-wise, flavour-wise, week-wise and festival-wise sales analytics. \n\nRefer Oracle\u2019s IoT architecture framework (Figure 1.5). IoT devices connect through the Internet. The data acquires, organises, processes and is analysed for the applications, services, enterprise applications, BPs and BI. The connectivity to the Internet of IoT/ M2M devices, such as sensors, streetlights, ATMs, RFIDs and automobiles, is for various applications and services. IoT/M2M applications, services and business processes use the messages and data packets received from the devices over the Internet. \n\nFollowing sections describe the data acquiring, organising, analytics, visualisations for IoT applications, services, business processes and knowledge discovery and management. \n\n\n![Image](/src/assets/generated_images/iot2_p186_i0.png)\n\n160 Internet of Things: Architecture and Design Principles \n\n5.2 DATA ACQUIRING AND STORAGE \n\nFollowing subsections describe devices data, and steps in acquiring and storing data for an application, service or business process. \n\n5.2.1 Data Generation \n\nData generates at devices that later on, transfers to the Internet through a gateway. Data generates as follows: \n\n\u25cfPassive devices data: Data generate at the device or system, following the result of interactions. A passive device does not have its own power source. An external source helps such a device to generate and send data. Examples are an RFID (Example 2.2) or an ATM debit card (Example 2.3). The device may or may not have an associated microcontroller, memory and transceiver. A contactless card is an example of the former and a label or barcode is the example of the latter. \n\n\u25cfActive devices data: Data generates at the device or system or following the result of interactions. An active device has its own power source. Examples are active RFID, streetlight sensor (Example 1.2) or wireless sensor node. An active device also has an associated microcontroller, memory and transceiver. \n\n\u25cfEvent data: A device can generate data on an event only once. For example, on detection of the traffic or on dark ambient conditions, which signals the event. The event on darkness communicates a need for lighting up a group of streetlights (Example 1.2). A system consisting of security cameras can generate data on an event of security breach or on detection of an intrusion. A waste container with associate circuit can generate data in the event of getting it filled up 90% or above. The components and devices in an automobile generate data of their performance and functioning. For example, on wearing out of a brake lining, a play in steering wheel and reduced air-conditioning is felt. The data communicates to the Internet. The communication takes place as and when the automobile reaches near a Wi-Fi access point. \n\n\u25cfDevice real-time data: An ATM generates data and communicates it to the server instantaneously through the Internet. This initiates and enables Online Transactions Processing (OLTP) in real time. \n\n\u25cfEvent-driven device data: A device data can generate on an event only once. Examples are: (i) a device receives command from Controller or Monitor, and then performs action(s) using an actuator. When the action completes, then the device sends an acknowledgement; (ii) When an application seeks the status of a device, then the device communicates the status. \n\n5.2.2 Data Acquisition \n\nData acquisition means acquiring data from IoT or M2M devices. The data communicates after the interactions with a data acquisition system (application). The application interacts \n\nApply the data-acquiring and data-storage functions for IoT/M2M \n\ndevices data and \n\nmessages \n\nLO 5.1 \n\nData Acquiring, Organising, Processing and Analytics 161 \n\nand communicates with a number of devices for acquiring the needed data. The devices send data on demand or at programmed intervals. Data of devices communicate using the network, transport and security layers (Figure 2.1). \n\nAn application can configure the devices for the data when devices have configuration capability. For example, the system can configure devices to send data at defined periodic intervals. Each device configuration controls the frequency of data generation. For example, system can configure an umbrella device to acquire weather data from the Internet weather service, once each working day in a week (Example 1.1). An ACVM can be configured to communicate the sales data of machine and other information, every hour. The ACVM system can be configured to communicate instantaneously in event of fault or in case requirement of a specific chocolate flavour needs the Fill service (Example 5.1). \n\nApplication can configure sending of data after filtering or enriching at the gateway at the data-adaptation layer. The gateway in-between application and the devices can provision for one or more of the following functions\u2014transcoding, data management and device management. Data management may be provisioning of the privacy and security, and data integration, compaction and fusion (Section 2.3). \n\nDevice-management software provisions for device ID or address, activation, configuring (managing device parameters and settings), registering, deregistering, attaching, and detaching (Section 2.3.2). \n\nExample 5.2 gives the process of acquiring data from the embedded component devices in the automobiles for Automotive Components and Predictive Automotive Maintenance System (ACPAMS) application. \n\nExample 5.2 \n\nProblem Internet of ACPAMS application\u2014How does an ACPAMS application acquire data from the embedded devices in the automobile components in a car? \n\nSolution \n\nA number of components, such as engine control system, axle, steering system, brake linings, wipers, air conditioners, battery and shockers, need predictive maintenance. Each component embeds computing hardware, software and interface to a network in an automobile. Each embedded device in the automobile communicates to a central computing system using the Controller Area Network (CAN) bus. Consolidated data then communicates thorough the Internet to the ACPAMS center (Example 5.2). An application at the system, first manages each embedded device. This means it allots the device ID (address), activates, configures (manages device parameters and settings), registers, deregisters, attaches and detaches. System gateway software communicates to a service-centre application. \n\nThe gateway application in event of an automobile in the vicinity of the hotspot, communicates the acquired data aggregated over preset required intervals for each embedded component device. The communication is through Wi-Fi (Section 2.2). As and when the automobile happens to be near a Wi-Fi hotspot, the device data communicates to the application. \n\nApplications configure \n\nthe devices for \n\nacquiring data \n\n162 Internet of Things: Architecture and Design Principles \n\nThe acquired data stores in the databases at a data store (Section 5.2.6). The application uses analytics and advanced analytics at scheduled intervals. The analytics predicts the maintenance needs for each automotive component after predefined intervals (Section 5.5). The application messages the automobile dashboard the needed preventive maintenances at regular intervals. \n\nSimilar to the above example, the industrial plants use applications for acquiring data from machines. Analytics of data enable necessary predictive or prescriptive maintenances (Section 5.5.1). \n\n5.2.3 Data Validation \n\nData acquired from the devices does not mean that data are correct, meaningful or consistent. Data consistency means within expected range data or as per pattern or data not corrupted during transmission. Therefore, data needs validation checks. Data validation software do the validation checks on the acquired data. Validation software applies logic, rules and semantic annotations. The applications or services depend on valid data. Then only the analytics, predictions, prescriptions, diagnosis and decisions can be acceptable. \n\nLarge magnitude of data is acquired from a large number of devices, especially, from machines in industrial plants or embedded components data from large number of automobiles or health devices in ICUs or wireless sensor networks, and so on. Validation software, therefore, consumes significant resources. An appropriate strategy needs to be adopted. For example, the adopted strategy may be filtering out the invalid data at the gateway or at device itself or controlling the frequency of acquiring or cyclically scheduling the set of devices in industrial systems. Data enriches, aggregates, fuses or compacts at the adaptation layer. \n\n5.2.4 Data Categorisation for Storage \n\nServices, business processes and business intelligence use data. Valid, useful and relevant data can be categorised into three categories for storage\u2014data alone, data as well as results of processing, only the results of data analytics are stored. Following are three cases for storage: \n\n1. Data which needs to be repeatedly processed, referenced or audited in future, and \n\ntherefore, data alone needs to be stored. 2. Data which needs processing only once, and the results are used at a later time using \n\nthe analytics, and both the data and results of processing and analytics are stored. Advantages of this case are quick visualisation and reports generation without reprocessing. Also the data is available for reference or auditing in future. 3. Online, real-time or streaming data need to be processed and the results of this \n\nprocessing and analysis need storage. \n\nData must be validated \n\nbefore storing \n\nData aggregation, \n\nadaptation and enrichment is done before communicating \n\nto the Internet \n\nData Acquiring, Organising, Processing and Analytics 163 \n\nData from large number of devices and sources categorises into a fourth category called Big data. Data is stored in databases at a server or in a data warehouse or on a Cloud as Big data. \n\n5.2.5 Assembly Software for the Events \n\nA device can generate events. For example, a sensor can generate an event when temperature reaches a preset value or falls below a threshold. A pressure sensor in a boiler generates an event when pressure exceeds a critical value which warrants attention. \n\nEach event can be assigned an ID. A logic value sets or resets for an event state. Logic 1 refers to an event generated but not yet acted upon. Logic 0 refers to an event generated and acted upon or not yet generated. A software component in applications can assemble the events (logic value, event ID and device ID) and can also add Date time stamp. Events from IoTs and logic-flows assemble using software. \n\n5.2.6 Data Store \n\nA data store is a data repository of a set of objects which integrate into the store. Features of data store are: \n\n\u25cfObjects in a data-store are modeled using Classes which are defined by the database schemas. \n\n\u25cfA data store is a general concept. It includes data repositories such as database, relational database, flat file, spreadsheet, mail server, web server, directory services and VMware \n\n\u25cfA data store may be distributed over multiple nodes. Apache Cassandra is an example of distributed data store. \n\n\u25cfA data store may consist of multiple schemas or may consist of data in only one scheme. Example of only one scheme data store is a relational database. Repository in English means a group, which can be related upon to look for required things, for special information or knowledge. For example, a repository of paintings of artists. A database is a repository of data which can be relied upon for reporting, analytics, process, knowledge discovery and intelligence. A flat file is another repository. \n\nFlat file means a file in which the records have no structural interrelationship (Section 5.3). Section 5.5.1 explains the spreadsheet concept. VMware uses data store to refer to a file that stores a virtual machine. \n\n5.2.7 Data Centre Management \n\nA data centre is a facility which has multiple banks of computers, servers, large memory systems, high speed network and Internet connectivity. The centre provides data security and protection using advanced tools, full data backups along with data recovery, redundant data communication connections and full system power as well as electricity supply backups. \n\nData centre is meant for data storage, data security and protection \n\n164 Internet of Things: Architecture and Design Principles \n\nLarge industrial units, banks, railways, airlines and units for whom data are the critical components use the services of data centres. Data centres also possess a dust free, heating, ventilation and air conditioning (HVAC), cooling, humidification and dehumidification equipment, pressurisation system with a physically highly secure environment. \n\nThe manager of data centre is responsible for all technical and IT issues, operations of computers and servers, data entries, data security, data quality control, network quality control and the management of the services and applications used for data processing. \n\n5.2.8 Server Management \n\nServer management means managing services, setup and maintenance of systems of all types associated with the server. A server needs to serve around the clock. Server management includes managing the following: \n\n\u25cfShort reaction times when the system or network is down \n\n\u25cfHigh security standards by routinely performing system maintenance and updation \n\n\u25cfPeriodic system updates for state-of-the art setups \n\n\u25cfOptimised performance \n\n\u25cfMonitoring of all critical services, with SMS and email notifications \n\n\u25cfSecurity of systems and protection \n\n\u25cfMaintaining confidentiality and privacy of data \n\n\u25cfHigh degree of security and integrity and effective protection of data, files and databases at the organisation \n\n\u25cfProtection of customer data or enterprise internal documents by attackers which includes spam mails, unauthorised use of the access to the server, viruses, malwares and worms \n\n\u25cfStrict documentation and audit of all activities. \n\n5.2.9 Spatial Storage \n\nConsider goods with RFID tags. When goods move from one place to another, the IDs of goods as well as locations are needed in tracking or inventory control applications. Spatial storage is storage as spatial database which is optimised to store and later on receives queries from the applications. \n\nSuppose a digital map is required for parking slots in a city. Spatial data refers to data which represents objects defined in a geometric space. Points, lines and polygons are common geometric objects which can be represented in spatial databases. Spatial database can also represent database for 3D objects, topological coverage, linear networks, triangular irregular networks and other complex structures. Additional functionality in spatial databases enables efficient processing. \n\nInternet communication by RFIDs, ATMs, vehicles, ambulances, traffic lights, streetlights, waste containers are examples of where spatial database are used. \n\nData Acquiring, Organising, Processing and Analytics 165 \n\nSpatial database functions optimally for spatial queries. A spatial database can perform typical SQL queries, such as select statements and performs a wide variety of spatial operations. Spatial database has the following features: \n\n\u25cfCan perform geometry constructors. For example, creating new geometries \n\n\u25cfCan define a shape using the vertices (points or nodes) \n\n\u25cfCan perform observer functions using queries which replies specific spatial information such as location of the centre of a geometric object \n\n\u25cfCan perform spatial measurements which mean computing distance between geometries, lengths of lines, areas of polygons and other parameters \n\n\u25cfCan change the existing features to new ones using spatial functions and can predicate spatial relationships between geometries using true or false type queries. \n\nReconfirm Your Understanding \n\n\u25cfData generates from the active or passive devices. Data can generate on events at devices. Data can also generate in real time. \n\n\u25cfAn application interacts for data acquisition. It can configure the devices for data when devices have configuration capability. For example, the devices can be configured to send data at defined periodic intervals, on specific events or in real time. \n\n\u25cfLarge magnitude of data acquired from a large number of devices, especially, from machines in industrial plants or embedded components data from large number of automobiles or health devices in ICUs or wireless sensor networks. Acquired data of the devices needs validation that data are correct, meaningful or consistent, are within expected ranges or are as per expected patterns and have not become corrupted during transmission. \n\n\u25cfData storage systems store the data. Data is stored in databases at a server, cloud, warehouse or as big data on the cloud. \n\n\u25cfData store is a data repository of a set of objects, which integrate into the store. \n\n\u25cfObjects in a data store are modelled using Classes, which the database schemas define. \n\n\u25cfData store includes database, relational database, flat file, spreadsheet, mail server, web server, directory services and VMware. Data store may be distributed over multiple nodes. \n\n\u25cfA data store may consist of multiple schemas or may consist of data in only one scheme, such as relational database. \n\n\u25cfData is stored on a server for short reaction times, optimised performance and high security. \n\n\u25cfA data centre stores data for data security and protection using the advanced tools, full data backups along with data recovery, redundant data communication connections and full system power. Data store requires data centre management or server management. \n\n\u25cfSpatial storage is storage through a spatial database which is optimised to store, enable querying of the data objects defined in a geometric space, and which is a database for 2D and 3D objects, topological coverage, linear networks, triangular irregular networks or other complex structures. \n\n\n![Image](/src/assets/generated_images/iot2_p192_i0.png)\n\n166 Internet of Things: Architecture and Design Principles \n\nSelf-Assessment Exercise \n\n1.  List the different types of data which is generated at the devices. 2. What does data acquisition mean? What are the benefits of data acquisition by \n\nan application after data aggregation, compaction or fusion and enrichment of data takes place from a number of devices? 3. What does data validation mean? When does a data acquisition application \n\nconsider data invalid? How can an application compensate for the missing or invalid data? 4. How does an application or service support software acquired data of \n\nindustrial plant machines? Show diagrammatically the in-between physical cum data-link, adaptation, network, transport layers. 5. What do you mean by data store? What are the different schemas for a data \n\nstore? 6. List the features of a data centre and the activities of a data centre manager. 7. What does server management mean? 8. What does spatial database mean? What are additional data fields that spatial \n\ndata possess? \n\n\u2605 \u2605\u2605 \n\n\u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605 \n\n\u2605\u2605 \n\n\u2605 \u2605\u2605\u2605 \n\n5.3 ORGANISING THE DATA \n\nData can be organised in a number of ways. For example, objects, files, data store, database, relational database and object oriented database. Following subsections describe these ways of organising and querying methods. \n\n5.3.1 Databases \n\nRequired data values are organised as database(s) so that select values can be retrieved later. \n\nDatabase \n\nOne popular method of organising data is a database, which is a collection of data. This collection is organised into tables. A table provides a systematic way for access, management and update. A single table file is called flat file database. Each record is listed in separate row, unrelated to each other. \n\nClassify ways \n\nof organising \n\ndata \n\nLO 5.2 \n\nNote: \u2605 Level 1 & Level 2 category \u2605\u2605 Level 3 & Level 4 category \u2605\u2605\u2605 Level 5 & Level 6 category \n\nData Acquiring, Organising, Processing and Analytics 167 \n\nRelational Database \n\nA relational database is a collection of data into multiple tables which relate to each other through special fields, called keys (primary key, foreign key and unique key). Relational databases provide flexibility. Examples of relational database are MySQL, PostGreSQL, Oracle database created using PL/SQL and Microsoft SQL server using T-SQL. \n\nObject Oriented Database (OODB) is a collection of objects, which save the objects in objected oriented design. Examples are ConceptBase or Cache. Example 5.3 shows the advantages of using relational databases. \n\nExample 5.3 \n\nProblem Recall Example 5.1. Show the advantages of relational databases taking the example of Internet of ACVMs. \n\nSolution \n\nA Manager application receives ACVMs information. It send requests to ACVMs for the chocolates sold, and number required. The request is sent each hour from Manager to Fill service. A Fill service executes on receipt of the requests from ACVMs for chocolate requirement every hour. Application, service and process use the common relational database RDBACVM tables. Table 5.1 is for ACVMs information, and pending service requests Num1, Num 2, Num2, Num 3 and Num 4 for the five flavours available at an ACVM. Table 5.2 is for ACVMs Fill Request Information, and Table 5.3 for chocolates Fill service actions (Tables 5.1 to 5.3). \n\nTable 5.1 RDBAVCM Table A\u2014ACVMs information \n\nMachine \n\nID \n\nRegion Address Installation \n\nDate \n\nMaintenance \n\nSchedule \n\nFill Service Address \n\nPending Request Number 1 \n\nPending Request Number 2 \n\nPending Request Number 3 \n\nPending Request Number 4 \n\nTable 5.2 RDBAVCM Table B\u2014ACVMs fill request information \n\nService Request Number \n\nMachine \n\nID \n\nRequest \n\nReceipt DateTime \n\nNumber FL1 Request \n\nNumber \n\nFL 2 Request \n\nNumber \n\nFL 3 Request \n\nNumber \n\nFL 4 Request \n\nNumber \n\nFL5 Request \n\nTable 5.3 RDBAVCM Table C\u2014ACVMs fill service actions \n\nService Request Number \n\nService DateTime \n\nNumber FL1 Sent \n\nNumber \n\nFL 2 Sent \n\nNumber \n\nFL 3 Sent \n\nNumber \n\nFL 4 Sent \n\nNumber \n\nFL5 Sent \n\n168 Internet of Things: Architecture and Design Principles \n\nCommon key fields between A, B and C are machine id or service request number. The relationships between fields of A, B and C are maintained by RDBMS. For example, when number of each flavour requested equals the number sent to a machine after processing a service request, then corresponding Num in A becomes 0. Assume three flat-file databases maintained separately\u2014A\u2019 for ACVMs information, B\u2019 for ACVMs Fill Service Request Information and database and C\u2019 for Fill Service Process. Then every time a service request raises or service request processes, corresponding fields of A\u2019, B\u2019 and C\u2019 in three flat file separate databases require update for maintenance of consistency of database in each application, service and process. Further, each one needs all fields due to no system for the maintenance of the relationships between A\u2019, B\u2019 and C\u2019. \n\nDatabase Management System \n\nDatabase Management System (DBMS) is a software system, which contains a set of programs specially designed for creation and management of data stored in a database. Database transactions can be performed on a database or relational database. \n\nAtomicity, Data Consistency, Data Isolation and Durability (ACID) Rules \n\nThe database transactions must maintain the atomicity, data consistency, data isolation and durability during transactions. Let us explain these rules using Example 5.3 as follows: \n\nAtomicity means a transaction must complete in full, treating it as indivisible. When a service request completes, then the pending request field should also be made zero. \n\nConsistency means that data after the transactions should remain consistent. For example, sum of chocolates sent should equal the sums of sold and unsold chocolates for each flavour after the transactions on the database. \n\nIsolation means transactions between tables 5.1 and 5.2, 5.2 and 5.3 and 5.3 and 5.1 are isolated from each other. \n\nDurability means after completion of transactions, the previous transaction cannot be recalled. Only a new transaction can affect any change. \n\nDistributed Database \n\nDistributed Database (DDB) is a collection of logically interrelated databases over a computer network. Distributed DBMS means a software system that manages a distributed database. The features of a distributed database system are: \n\n\u25cfDDB is a collection of databases which are logically related to each other. \n\n\u25cfCooperation exists between the databases in a transparent manner. Transparent means that each user within the system may access all of the data within all of the databases as if they were a single database. \n\n\u25cfDDB should be \u2018location independent\u2019, which means the user is unaware of where the data is located, and it is possible to move the data from one physical location to another without affecting the user.4 \n\n4 http://www.csee.umbc.edu/portal/help/oracle8/server.815/a67784/ds_ch1.htm \n\nData Acquiring, Organising, Processing and Analytics 169 \n\nConsistency, Availability and Partition-Tolerance Theorem \n\nConsistency, Availability and Partition-Tolerance Theorem (CAP theorem) is a theorem for distributed computing systems. The theorem states that it is impossible for a distributed computer system to simultaneously provide all three of the Consistency, Availability, Partition tolerance (CAP) guarantees.5 This is due to the fact that a network failure can occur during communication among the distributed computing nodes. Partitioning of a network therefore needs to be tolerated. Hence, at all times either there will be consistency or availability. \n\nConsistency means \u2018Every read receives the most recent write or an error\u2019. When a message or data is sought the network generally issues notification of time-out or read error. During an interval of a network failure, the notification may not reach the requesting node(s). \n\nAvailability means \u2018Every request receives a response, without guarantee that it contains the most recent version of the information\u2019. Due to the interval of network failure, it may happen that most recent version of message or data requested may not be available. \n\nPartition tolerance means \u2018The system continues to operate despite an arbitrary number of messages being dropped by the network between the nodes\u2019. During the interval of a network failure, the network will have two separate set of networked nodes. Since failure can always occur therefore, the partitioning needs to be tolerated. \n\n5.3.2 Query Processing \n\nQuery means an application seeking a specific data set from a database. For example, a query at a relational database at bank server may be for the ATM transactions made in a month by a specific customer ID (Example 2.3). Other examples are: most-liked chocolate flavour in the city by children of age group 6 to 10 (Example 5.1); number of times a vehicle visited at the ACPAMS center (Example 5.2) and service was rendered with satisfaction level of 5 out of 5. \n\nQuery Processing \n\nQuery processing means using a process and getting the results of the query made from a database. The process should use a correct as well as efficient execution strategy. Five steps in processing are: \n\n1. Parsing and translation: This step translates the query into an internal form, into \n\na relational algebraic expression and then a Parser, which checks the syntax and verifies the relations. 2. Decomposition to complete the query process into micro-operations using the analysis \n\n(for the number of micro-operations required for the operations), conjunctive and disjunctive normalisation and semantic analysis. 3. Optimisation which means optimising the cost of processing. The cost means number \n\nof micro-operations generated in processing which is evaluated by calculating the costs of the sets of equivalent expressions. \n\n5 https://en.wikipedia.org/wiki/CAP_theorem \n\n170 Internet of Things: Architecture and Design Principles \n\n4. Evaluation plan: A query-execution engine (software) takes a query-evaluation plan \n\nand executes that plan. 5. Returning the results of the query. \n\nThe process can also be based on a heuristic approach, by performing the selection and projection steps as early as possible and eliminating duplicate operations. \n\nDistributed Query Processing \n\nDistributed Query Processing means query processing operations in distributed databases on the same system or networked systems. The distributed database system has the ability to access remote sites and transmit the queries to other systems. \n\n5.3.3 SQL \n\nSQL stands for Structured Query Language. It is a language for viewing or changing (update, insert or append or delete) databases. It is a language for data querying, updating, inserting, appending and deleting the databases. It is a language for data access control, schema creation and modifications. It is also a language for managing the RDBMS. \n\nSQL was originally based upon the tuple relational calculus and relational algebra. SQL can embed within other languages using SQL modules, libraries and pre-compilers. SQL features are as follows: \n\n\u25cfCreate Schema is a structure that contains descriptions of objects created by a user (base tables, views, constraints). The user can describe and define the data for a database. \n\n\u25cfCreate Catalog consists of a set of schemas that constitute the description of the database. \n\n\u25cfUse Data Definition Language (DDL) for the commands that depict a database, including creating, altering and dropping tables and establishing constraints. The user can create and drop databases and tables, establish foreign keys, create view, stored procedure, functions in a database. \n\n\u25cfUse Data Manipulation Language (DML) for commands that maintain and query a database. The user can manipulate (INSERT, UPDATE or SELECT the data and access data in relational database management systems. \n\n\u25cfUse Data Control Language (DCL) for commands that control a database, including administering privileges and committing data. The user can set (grant or add or revoke) permissions on tables, procedures, and views. \n\n5.3.4 NOSQL \n\nNOSQL stands for No-SQL or Not Only SQL that does not integrate with applications that are based on SQL. NOSQL is used in cloud data store. NOSQL may consist of the following: \n\n\u25cfA class of non-relational data storage systems, flexible data models and multiple schemas \n\nData Acquiring, Organising, Processing and Analytics 171 \n\n\u25cfClass consisting of uninterpreted key and value or \u2018the big hash table\u2019. For example in [Dynamo (Amazon S3)] \n\n\u25cfClass consisting of unordered keys and using the JSON. For example in PNUTS \n\n\u25cfClass consisting of ordered keys and semi-structured data storage systems. For examples in the BigTable, Hbase and Cassandra (used in Facebook and Apache) \n\n\u25cfClass consisting of JSON (Section 2.3). For example in MongoDb6 which is widely used for NOSQL) \n\n\u25cfClass consisting of name and value in the text. For example in CouchDB \n\n\u25cfMay not require a fixed table schema NOSQL systems do not use the concept of joins (in distributed data storage systems). Data written at one node replicates to multiple nodes, therefore identical and distributed system can be fault-tolerant, and can have partitioning tolerance. CAP theorem is applicable. The system offers relaxation in one or more of the ACID and CAP properties. Out of the three properties (consistency, availability and partitions), two are at least present for an application. \n\n\u25cfConsistency means all copies have same value like in traditional DBs. \n\n\u25cfAvailability means at least one copy available in case a partition becomes inactive or fails. For example, in web applications, the other copy in other partition is available. \n\n\u25cfPartition means parts which are active but may not cooperate as in distributed databases. \n\n5.3.5 Extract, Transform and Load \n\nExtract, Transform and Load or ETL is a system which enables the usage of databases used, especially the ones stored at a data warehouse. Extract means obtaining data from homogeneous or heterogeneous data sources. Transform means transforming and storing the data in an appropriate structure or format. Load means the structured data load in the final target database or data store or data warehouse. \n\nAll the three phases can execute in parallel. Data extraction takes longer time. Therefore, the system while pulling data, executes another transformation processes on already received data and prepares the already transformed data for loading. As soon as data are ready for load into the target, the data load starts. It means next phase starts without waiting for the completion of the previous phases. \n\nETL system usages are for integrating data from multiple applications (systems) hosted separately. \n\n5.3.6 Relational Time Series Service \n\nTime series data means an array of numbers indexed with time (date-time or a range of date-time). Time series data can be considered as time stamped data. It means data carries along with it the date and time information about the data values. For example, sales of chocolates in Internet of ACVMs (Example 5.1) are different on different dates and times. \n\n6 http://www.w3resource.com/mongodb/introduction-mongodb.php MongoDb \n\n172 Internet of Things: Architecture and Design Principles \n\nThe sales need indexing with a range between two dates or indexed with date-time. A time series of sales is called sale profile of the ACVMs. A time series of log of chocolate sales is called chocolate sales trace. \n\nTime series is any data-set that is accessed in a sequence of time. Software programs and an analytics program analyses the set in a time series, meaning analyses in a chronological order. IoT devices, such as temperature sensors, wireless sensor network nodes, energy meters, RFID tags, ATMs, ACVMs generate time-stamped or time series data. \n\nTime Series Database (TSDB) is a software system which implements a database that optimally handles mathematical operations (profiles, traces, curves), queries or database transactions on time series. \n\nConventional database systems, Relational Database System (RDMS) or flat file database software may not be modelled for time series handling and may not therefore function efficiently for time series data with complex logic or business rules and need of high transaction throughout time series data. \n\nIBM Informix TimeSeries software expanded database functionality by adding efficient storage, faster load of data to enable fast query processing and transactions, fast performance, sophisticated support for managing time series. Server can have built-in time series Informix software for the handling of IoT time series data. \n\n5.3.7 Real-Time and Intelligence \n\nDecision on real-time data is fast when query processing in live data (streaming) has low latency. Decision on historical data is fast when interactive query processing has low latency. Low latencies are obtained by various approaches: Massively Parallel Processing (MPP), in-memory databases and columnar databases. \n\nTeraData Aster and Pivotal Greenplum are examples of MPP. In-memory and on-store both transaction methods exist for the databases. SAP Hana and QClick view are examples of in-memory databases. SAP Sybase IQ and HP Vertica are examples for columnar databases for faster Analytics. \n\nReconfirm Your Understanding \n\n\u25cfA database is a collection of data which is organised as tables. A relational database is a collection of data organised as multiple tables, which relate to each other through special fields. Object Oriented Database (OODB) is a collection of objects, which saves the objects in objected oriented design. \n\n\u25cfDatabase Management System is a software system, which contains a set of programs specially designed for creation, management and transactions of data stored in a database. \n\n\u25cfDatabase transaction is the execution of a specific set of operations on a database. Relational database transaction is the execution of interrelated instructions using relations. A transaction is a sequential execution of a specific set of relational operations on relational database. \n\nData Acquiring, Organising, Processing and Analytics 173 \n\n\u25cfDatabase transaction models state that transactions must maintain transaction atomicity, data consistency, data isolation and durability. \n\n\u25cfQuery means to an application or service seeking a specific data set from a database. Query processing means using a process and getting the results of the query made from a database. \n\n\u25cfCAP theorem applies to distributed computing nodes. CAP theorem states that for distributed computing systems, it is impossible for a distributed computer system to simultaneously provide all three, consistency, availability, partition tolerance (CAP), guarantees. \n\n\u25cfDistributed database is a collection of logically interrelated, cooperating databases over a computer network. A distributed database system has the ability to access remote sites and transmit queries. \n\n\u25cfDistributed query processing means query processing operations in distributed databases on same system or networked systems. \n\n\u25cfSQL is a language for data access control, schema creation and modifications. It is a language for managing the RDBMs. It is a language for data definition, data manipulation and data control instructions. \n\n\u25cfNOSQL stands for Not Only SQL or No-SQL and no integration with applications that are based on SQL. It is used in Cloud Data Store. NOSQL consists of classes of non-relational data storage systems, flexible data models and multiple schemas. \n\n\u25cfTime series data means an array of numbers indexed with time (date-time or a range of date-time). \n\n\u25cfDecision on real-time data is fast when query processing in live data (streaming) has low latency. The decision on historical data is fast when interactive query processing has low latency. \n\nSelf-Assessment Exercise \n\n1. What does a relational database mean? 2. List the differences between flat-file and relational databases. 3. Consider relational database tables, A, B and C. Here is A bank information \n\n(name, address, phone numbers, IFSC code), B is customers\u2019 information (IDs, names, addresses, phone numbers, Account Types, Account Numbers) and C is Bank Passbook Transactions details (Date of Transaction, Credit, debit, Debit amount, Credit amount and Balance). What will be the keys used? How does the data in the tables relate? C relates to which fields in A and B. 4. What are three essential features when using distributed databases? 5. What does TSDB mean? 6. What are the features of SQL? 7. How does SQL differ from NOSQL? 8. List the differences between time-series database system and RDBMS in \n\nconstruction and usages. \n\n\u2605 \u2605\u2605 \u2605\u2605\u2605 \n\n\u2605 \u2605 \u2605 \u2605\u2605 \u2605\u2605\u2605 \n\n\n![Image](/src/assets/generated_images/iot2_p200_i0.png)\n\n174 Internet of Things: Architecture and Design Principles \n\n5.4  TRANSACTIONS, BUSINESS PROCESSES, \n\nINTEGRATION AND ENTERPRISE SYSTEMS \n\nA transaction is a collection of operations that form a single logical unit. For example, a database connect, insertion, append, deletion or modification transactions. Business transactions are transactions related in some way to a business activity. \n\n5.4.1 Online Transactions and Processing \n\nRecall Example 2.3\u2014OLTP means process as soon as data or events generate in real time. OLTP is used when requirements are availability, speed, concurrency and recoverability in databases for real-time data or events. Example 5.4 gives the uses of OLTP in the application and network domain in Internet of ATMs (ATM of a bank) connected to a bank server. \n\nExample 5.4 \n\nProblem What are the usages of OLTP in the application and network domain in Internet of ATMs (ATM of a bank) connected to a bank server? \n\nSolution \n\nServer applications need processing and update-intensive database management with a high throughput. The requirements in these applications are availability, speed, concurrency and recoverability, and reduced paper trails. Therefore, the transactions at ATMs need OLTP. \n\nBatch Transactions Processing \n\nBatch transactions processing means the execution of a series of transactions without user interactions. Transaction jobs are set up so they can be run to completion. Scripts, command-line arguments, control files, or job control language predefine all input parameters. \n\nBatch processing means a transaction process in batches and in an non-interactive way. When one set of transactions finish, the results are stored and a next batch is taken up. A good example is credit card transactions where the final results at the end of the month are used. Another example is chocolate purchase transactions. The final results of sell figures from ACVMs can communicate on the Internet at the end of an hour or day. \n\nSummarise the transactions on stored data, functions \n\nfor business- processes and business intelligence, \n\nand the concepts of \n\nIoT applications\u2014 integration and services \n\narchitecture \n\nLO 5.3 \n\nData Acquiring, Organising, Processing and Analytics 175 \n\nStreaming Transactions Processing \n\nExamples of the streams are log streams, event streams and twitter streams. Query and transactions processing on streaming data need specialised frameworks. Storm from Twitter, S4 from Yahoo, SPARK streaming, HStreaming and flume are examples of frameworks for real-time streaming computation frameworks. \n\nInteractive Transactions Processing \n\nInteractive transactions processing means the transactions which involve continual exchange of information between the computer and a user. For example, user interactions during e-shopping and e-banking. The processing is just the opposite of batch processing. \n\nReal-time Transactions Processing \n\nReal-time transaction processing means that transactions process at the same time as the data arrives from the data sources and data store. An example is ATM machine transactions. In-memory, row-format records enable real-time transaction processing. Row format means few rows and more columns. The CPU accesses all columns in single accesses in SIMD (single instruction multiple data) streams processing. \n\nEvent Stream Processing and Complex Event Processing \n\nEvent Stream Processing (ESP) is a set of technologies, event processing languages, Complex Event Processing (CEP), event visualisation, event databases and event-driven middleware. Apache S4 and Twitter Storm are examples of ESPs. SAP Sybase ESP and EsperTechEsper are examples of CEPs. ESP and CEP does the following: \n\n\u25cfProcesses tasks on receiving streams of event data \n\n\u25cfIdentifies the meaningful pattern from the streams \n\n\u25cfDetects relationships between multiple events \n\n\u25cfCorrelates the events data \n\n\u25cfDetects event hierarchies \n\n\u25cfDetects aspects such as timing, causality, subscription membership \n\n\u25cfBuilds and manages the event-driven information systems. \n\nComplex Event Processing \n\nCEP has many applications. For example, IoT event processing applications, stocks algorithmic-based trading and location-based services. A CEP application in Eclipse are used for capturing a combination of data, timing conditions and efficiently recognise the corresponding events over data streams. \n\n5.4.2 Business Processes \n\nA business process consists of a series of activities which serves a particular specific result. It is used when an enterprise has a number of interrelated processes which serve \n\n176 Internet of Things: Architecture and Design Principles \n\na particular result or goal. The results enable sales, planning and production. The BP is a representation or process matrix or flowchart of a sequence of activities with interleaving decision points. \n\nInternet of RFIDs enables a business process called tracking of RFID labelled goods (Example 2.2) which also enables inventory control process. \n\nIoT/M2M enables the devices\u2019 data in databases for business processes. The data supports the process. For example, consider a process, streetlights control and management (Example 1.2). Each group of streetlights sends data in real time through the gateways. The gateways connect to the Internet. The control and management processes streetlights real time databases and group databases. \n\n5.4.3 Business Intelligence \n\nBusiness intelligence is a process which enables a business service to extract new facts and knowledge and then undertake better decisions. The new facts and knowledge follow from the earlier results of data processing, aggregation and then analysing those results. Example 5.5 shows business intelligence for Internet of ACVMs, whereas Example 5.6 shows business processes, intelligence and BP architecture reference model in Automotive Maintenance Application at the Service Centre. \n\nExample 5.5 \n\nProblem Recall Internet of ACVMs (Example 5.1). What are the new facts and knowledge that follow from the earlier results of business processes? \n\nSolution \n\nConsider Fill service for the ACVMs. The BI extracts the knowledge about the required Fill service frequency from the facts. Also extract the strategy to service the filling of chocolates in ACVMs in specific areas of the city such that all flavours can be simultaneously dispatched. BI lies in the service to the machines in the area when cost incurred is minimum on one hand and each machine is able to serve on user demand any flavour without fail. \n\nExample 5.6 \n\nProblem Recall automotive maintenance application at a service centre (Example 5.2). Draw BI/BP architecture for automobile components service process at ACPAMS. \n\nSolution \n\nThe predictive analytics of the acquired data in databases enables the service to extract the knowledge. It gets the prediction of components needing service from the facts. The service to a set of automobile components has to be timely and preventive. BI lies in the service to the automobile when components are serviced or replaced timely with least number of visits to the service centre. Figure 5.1 shows model architecture for BI and BP at automobile service centre. \n\nData Acquiring, Organising, Processing and Analytics 177 \n\nGateway and Sources Acquiring Data Layers \n\nAdaptation and Enrichment \n\nIoT/M2M Data Sources Automobile Components Layers \n\nOrganised Data Store Layer \n\nBusiness Analytics and Intelligence Applications \n\nApplications Support for Analytics \n\nAutomobile Specific Database and Component Specific Historical Databases Automobile Components Data Integration \n\nBusiness Intelligence \n\nData Access, SQL, Query Processing, R-Descriptive Statistics, Predictive Analytics \n\nService Centre Business Processes \n\nFigure 5.1  Architecture reference model for the business intelligence and business processes at \n\nACPAMS \n\n5.4.4 Distributed Business Process \n\nSeveral times, business processes need to be distributed. Distribution of processes reduces the complexity, communication costs, enables faster responses and smaller processing load at the central system. For example, recall Example 1.2, distribution of control process for each group of lights at the gateway itself reduces complexity, communication costs, faster responses and smaller processing load at the central system. \n\nDistributed Business Process System (DBPS) is a collection of logically interrelated business processes in an Enterprise network. DBPS means a software system that manages the distributed BPs. DBPS features are: \n\nDBPS is a collection of logically related BPs like DDBS. DBPS exists as cooperation between the BPs in a transparent manner. Transparent means that each user within the system may access all of the process decisions within all of the processes as if they were a single business process. \n\nDBPS should possess \u2018location independence\u2019 which means the enterprise BI is unaware of where the BPs are located. It is possible to move the results of analytics and knowledge from one physical location to another without affecting the user.1 \n\nExample 5.7 shows distributed business processes in an automobile enterprise. \n\n178 Internet of Things: Architecture and Design Principles \n\nExample 5.7 \n\nProblem Recall automotive maintenance application at an ACPAMS centre (Examples 5.2 and 5.6). Enterprise business intelligence lies in predictive and prescriptive analytics based services to the automobile components, which are serviced or replaced timely with minimum number of visits to service centres. Draw business intelligence and business processes architecture for automobile components service process. \n\nSolution \n\nFigure 5.2 shows model architecture for distributed BI and BP at an automobile enterprise. Two business processes are at enterprise layer, viz. one at network layer and one at devices and gateway layer. \n\nAdaptation, Data Integration and Enrichment \n\nIoT/M2M Data Sources Automobile Components Device Layers \n\nAutomobile Enterprise \n\nBusiness Process 1 \n\nAnalytics Automobile Specific \n\nDatabase \n\nBusiness Intelligence, Predictive Analytics \n\nAutomobiles Database Business Process 3 \n\nNetwork Layer Data Access, SQL, Query Processing, R-Descriptive Statistics, and Component Specific Historical Databases Business Process 2 \n\nAutomobile Components \n\nDevice and Gateway Layer Business Process 4 \n\nFigure 5.2  Distributed interrelated business intelligence and processes at the enterprise, network, and \n\ndevices and gateway layers \n\nInterrelationships in distributed BPs are: \n\n\u25cfEnterprise layer business process 1 (EBP1) interrelates directly with the device and gateway layer Business process 4 (DGBP4) thus with the device data, adaptation, data integration and enrichment layer for a specific automobile. EBP1 analytics is with non-historical data. \n\n\u25cfNetwork layer Business Process 2 (NBP2) interrelates directly with the enterprise layer business process 3 (EBP3). NBP2 uses the data access, SQL, query processing, R- descriptive statistics and component specific historical databases. NBP 2 has access to data of number of automobiles of same the model as one sending data to EBP1. EBP3 analytics is with the other automobile databases and historical databases, and enables predictive and prescriptive analytics. \n\n\u25cfNBP2 interrelates directly with the DGBP4. This enables updating the database for the NBP2. \n\nData Acquiring, Organising, Processing and Analytics 179 \n\n5.4.5 Complex Applications Integration and Service Oriented Architecture \n\nAn enterprise has number of applications, services and processes. Heterogeneous systems have complexity when integrating them in the enterprise. \n\nFollowing are the standardised business processes, as defined in the Oracle application integration architecture: \n\n\u25cfIntegrating and enhancing the existing systems and processes \n\n\u25cfBusiness intelligence \n\n\u25cfData security and integrity \n\n\u25cfNew business services and products (web services) \n\n\u25cfCollaboration and knowledge management \n\n\u25cfEnterprise architecture and SOA \n\n\u25cfe-commerce \n\n\u25cfExternal customer services \n\n\u25cfSupply chain automation and analytics results visualisation \n\n\u25cfData centre optimisation IoT applications, services and processes enhance the existing systems in a number of enterprises. For example, an automobile enterprise has a number of divisions. Each division has Sales, Customer Relations Management, Automobile Maintenance Services, and Accounting. IoT-based services help in business intelligence, processes and systems, such as post-sales services and supply chain automation and analytics results in visualisation enhancement of the services from an enterprise. \n\nComplex application integration means integration of heterogeneous application architectures and number of processes. SOA consists of services, messages, operations and processes. \n\nSOA components distribute over a network or the Internet in a high-level business entity. New business applications can be developed using a SOA. \n\n5.4.6 Integration and Enterprise Systems \n\nFigure 5.3 shows complex applications integration architecture and SOA of cloud-based IoT services, web services, cloud services and services. \n\nProcess orchestration means a number of business processes running in parallel and a number of processes running in sequence. The process matrix provides the decision points which indicate which processes should run in parallel and which in sequence. An SOA models the number of services and interrelationships. Each service initiates on receipt of messages from a process or service. \n\nThe service discovery and selection software components select the services for application integration. Service orchestration software coordinates the execution of the number of services, cloud services, cloud IoT services and web services. Services run in parallel and a number of processes in sequences. \n\n180 Internet of Things: Architecture and Design Principles \n\nWeb Service Web Service \n\nIoT Service Service \n\nCloud of Things \n\nService A Service B \n\nService Discovery, Selection and Orchestration \n\nCloud \n\nService C Service D \n\nEnterprise \n\nBusiness Process \n\nDecision Points Business Process \n\nBusiness Processes Orchestration \n\nBusiness Process \n\nBusiness Process Business Process \n\nFigure 5.3  Complex applications integration architecture and SOA of cloud-based IoT services, web \n\nservices, cloud services and services \n\nReconfirm Your Understanding \n\n\u25cfA database transaction is a collection of operations that form a single logical unit of database. A transaction means operations such as connect, insertion, append, deletion or modification in a unit of database. Business transactions are transactions related in some way to a business activity. \n\n\u25cfOLTP stands for Online Transactions Processing, refers to processing of data or events in real time. Batch transaction processing means transaction processing in batches and in a non-interactive way. Stream transactions processing on streaming data need specialised frameworks. Real-time transaction processing means that transaction processing is done at the same time as the data arrives from the data sources and data stores. \n\n\u25cfComplex event processing application uses are for capturing a combination of data, timing conditions and efficiently recognising the corresponding events over data streams. \n\n\u25cfA business process consists of a series of activities. The process may consist of a collection of interrelated structured activities or tasks or processes which follow a logical sequence. \n\n\u25cfA process matrix has a number of elements. Each element may represent a series of operations and activities on a given set of inputs that perform a specific task leading to a decision point in the process. \n\n\n![Image](/src/assets/generated_images/iot2_p207_i0.png)\n\nData Acquiring, Organising, Processing and Analytics 181 \n\n\u25cfBusiness intelligence is a process that enables a business service to extract new facts and knowledge and then undertake better decisions. The new facts and knowledge follow from the earlier results of data processing, aggregation and then analysing those results. \n\n\u25cfDistribution of processes reduces complexity, communication costs, and enables faster responses and smaller processing load at the central system. \n\n\u25cfDistributed business process system (DBPS) is a collection of logically interrelated business processes in an enterprise network. \n\n\u25cfComplex application integration means integration of heterogeneous application architectures and number of processes. \n\n\u25cfSOA consists of enterprise and service discovery, selection and orchestration layers for services, messages, operations and processes. \n\nSelf-Assessment Exercise \n\n1. What do batch transactions, streaming transactions and real-time transactions \n\nprocessing mean? 2. List the tasks which event stream processing and complex stream processing \n\ndo. 3. What does a process matrix mean? 4. List the steps in tracking a business process in Internet of RFIDs. Draw \n\ndiagrammatically the actions at physical cum data-link, data adaptation, network, transport, application-support and application layers. 5. What is the benefit of device and gateway layer business process in distributed \n\nbusiness processes in automobile enterprise? 6. List the standardised business processes in the Oracle application integration \n\narchitecture. 7. Why does OLTP operation run fast in row format? 8. Draw waste container management business intelligence and business \n\nprocesses architecture for Internet of Waste containers. Assume that each container generates an event on getting 90% filled up and communicates the event along with the container ID. \n\n\u2605\u2605 \n\n\u2605 \n\n\u2605 \u2605\u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605\u2605 \n\n\u2605 \u2605\u2605\u2605 \n\n5.5 ANALYTICS \n\nOrganised data after acquiring from the devices can be used for multiple purposes. Applications usually use the data of devices in two ways\u2014for monitoring, reporting and rule-based actions. For example, in Internet of Streetlights applications just do that (Example 1.2); for analytics, new \n\nIdentify the functions and usage of data analytics and data visualisations for IoT applications and \n\nbusiness processes \n\nLO 5.4 \n\n182 Internet of Things: Architecture and Design Principles \n\nfacts and taking decisions based on those facts. For example, Internet of ACVMs can use analytics, new facts are found and those facts enable taking of the decisions for new option(s) to maximise the profits from the machines (Example 5.1). \n\nExample 5.8 gives the uses of analytics in the application and network domain in Internet of ATMs (ATM of a bank) connected to a bank server. \n\nExample 5.8 \n\nProblem What are the usages of analytics in the application and network domain in Internet of ATMs connected to a bank server? \n\nSolution \n\nAn ATM machine generates income from transaction charges, charges for advertisements at the machine locations and idle-state advertisements of bank products and bank services at the display screen. Each machine\u2019s usage is analysed during each hour on a daily basis in each segment. Each machine\u2019s expenses and incomes are used for cost-benefit analysis. Analytics enable faster and more accurate scheduling. It enables timely actions, enables optimum scheduling region wise for the cash supply service, special scheduling on special days, such as days near dates of receiving salary, festivals and holidays. The analytics also enable each region machines the maintenance, scheduling and ATM location relocation. \n\nAn enterprise creates sections and unit-wise analytics. The analytics enable fact-based decision making in place of intuition-drive decision making. Analytics provides business intelligence. It is a key for the success of an enterprise business. \n\nAnalytics require the data to be available and accessible. It uses arithmetic and statistical, data mining and advanced methods, such as machine learning to find new parameters and information  which add value to the data. Analytics enable building models based on selection of right data. Later the models are tested and used for services and processes. \n\n5.5.1 Analytics Phases \n\nAnalytics has three phases before deriving new facts and providing business intelligence. These are: \n\n1. Descriptive analytics enables deriving the additional value from visualisations and \n\nreports. 2. Predictive analytics is advanced analytics which enables extraction of new facts and \n\nknowledge, and then predicts or forecasts. 3. Prescriptive analytics enables derivation of the additional value and undertake better \n\ndecisions for new option(s) to maximise the profits. \n\nDescriptive Analytics \n\nDescriptive analytics answers the questions about what happened in the past. Descriptive analytics means finding the aggregates, frequencies of occurrences, mean values (simple \n\nData Acquiring, Organising, Processing and Analytics 183 \n\nor geometric averages) or variances in values or groupings using selected properties and hence applying these. Descriptive analytics enable the following: \n\n\u25cfActions, such as Online Analytical Processing (OLAP) for the analytics \n\n\u25cfReporting or generating spreadsheets \n\n\u25cfVisualisations or dashboard displays of the analysed results \n\n\u25cfCreation of indicators, called key performance indicators. \n\nDescriptive Analytics Methods \n\n\u25cfSpreadsheet-based reports and data visualisations: Results of descriptive analysis can be presented in a spreadsheet format before creating the data visuals for the user. Spreadsheet enables user visualisation of what if. For example, if sales of chocolates of specific flavour drop by 5% on specific set of ACVMs, how it will influence the profitability? A spreadsheet is a table. The values are in the cells in the rows and columns. Each value can have a predefined relationship to the other values. For example, a value in cell CjRi (cell at jth column and ith row) can be related to another cell or a set of cells through a formula or Boolean relation or statistically analysed value. \n\n\u25cfDescriptive statistics-based reports and data visualisations: Descriptive analysis can also use descriptive statistics. Statistical analysis means finding peak, minima, variance, probabilities, and statistical parameters. Formulae are used for the data sets to enable the data showing variations understandable. \n\n\u25cfData mining and machine learning methods in analytics: Data mining analysis means use of algorithms which extract hidden or unknown information or patterns from large amounts of data. Machine learning means modelling of the specific tasks. \n\nR is a programming language and software environment for statistical computing and graphics. The language is also the core of many open source products. Descriptive analytics enable intelligence for further actions. \n\nExample 5.9 \n\nProblem \n\na.  Recapitulate Examples 5.1, 5.3 and 5.5. How are the descriptive analytics used in Internet of ACVMs? b. How is the spreadsheet method used in Internet of ACVMs? \n\nSolution \n\na. Descriptive analytics looks at past performance and the performance is evaluated by mining historical \n\ndata. The analytics find the reasons behind past performance or success or failure. Reports for management use this type of analysis. For example, report the analysis of the sales in individual areas, sales on individual occasions, analysing children preferences, analysing flavour preferences, expenses and incomes from individual regions, incomes from various sources, chocolate sales, advertisements during idle state displays at the machines, or chocolate sales on the machines, and so on. b. Recall Example 5.3. Spreadsheet design is as follows: A cell in a row is for an ACVM id and another \n\nfor the period under consideration. Five other cells are for the numbers sold during the period for the \n\n184 Internet of Things: Architecture and Design Principles \n\nflavours FL1 and FL5. A predefined formula calculates the profitability from the values in the cells and values of the purchase and sell-prices for each flavour, FL1 and FL5. If a value given in one of the five cell changes then new profitability figure automatically calculates using the predefined formulae in each row and the summing row. The spreadsheet analytics can be graphically visualised. \n\n\u25cfOnline analytical processing (OLAP) in analytics: OLAP enables viewing of analysed data up to the desired granularity. It enables view of rollup (finer granulites data to coarse granulites data) or drill down (coarser granulites data to finer granulites data). OLAP enables obtaining summarized information and automated reports from large volume database. Results of queries are based on Metadata. Metadata is data which describes the data. Pre-storing calculated values provide consistently fast response. \n\nOLAP uses the analysis functions which are not possible to code in SQL. The data structure is designed from the users perspective, using Spreadsheet like formulae. \n\nOLAP is a significant improvement over query systems. OLAP is an interactive system to show different summaries of multidimensional data by interactively selecting the attributes in a multidimensional data cube.7 \n\nOLAP enables analysing data in multiple dimensions in a structure called data cube. Each dimension represents a hierarchy. Each dimension has a dimension attribute which defines the dimension and summary of measure attribute. \n\nA slice of a data-cube can be viewed when values of multiple dimensions are fixed. A dice of a data-cube can be viewed with variable values in multiple dimensions. Slicing and dicing functionalities mean selecting specific values for these attributes, which are then displayed on top of the cross-tables. \n\nA slice means a data relationship in the analysed multiple dimensional data. A slice of a data relationship between two attributes can be individually visualised. For examples, monthly sales versus flavours sold at the chain of ACVMs in Example 5.1 after the analysis. \n\nA cubical dice has six faces, each face marked distinctly. Face 1 has one dot, face 2 two, and so on. Sixth face has six dots. Similarly, six different cross referenced tables can be created during OLAP for three-dimensional structure for analysing data. An n-dimensional structure will have 2-n faces (tables). Each table and corresponding visual gives a relationship between two attributes. The tables are cross referenced. OLAP can be one of the three types: multidimensional OLAP (MOLAP), relational OLAP (ROLAP) and hybrid OLAP (HOLAP). \n\nExample 5.10 explains multidimensional data-cube analytics. \n\n7 https://en.wikipedia.org/wiki/Online_analytical_processing \n\nDicing is a process \n\nof creating cross referenced tables, each \n\nviewable separately on n-dimensional structure \n\nfaces. \n\nData Acquiring, Organising, Processing and Analytics 185 \n\nExample 5.10 \n\nProblem How are the OLAP used for analytics in Internet of ACVMs (Example 5.1)? \n\nSolution \n\nConsider Internet of ACVMs (Example 5.1). First dimension can be for time intervals, varying in hierarchy from hour, day, week, month and year. Second dimension can be number of installed machines with hierarchically aggregated values from 10s, 50s, 100s, and so on. The third dimension can be number of chocolates sold, with hierarchically aggregated values, 100s, 1000s, 10000s, and so on. Fourth dimension can be number of individual chocolate flavours sold, 50s, 100s, 150s, 200s and so on of a specific flavour. Similarly, fifth, sixth and other dimensions can be specified for other flavours. OLAP uses the following steps: Identify the dimensions, each with an attribute and hierarchy. For example, identifying dimensions: 1 number of time intervals, 2 number of installed machines, 3 total number of chocolates of all five flavours sold. Analyse cross tabulations (Row header with one attribute, column header with another attribute and cell having the aggregate value or calculated value according to a formula or analysis). For example, table of number of chocolates sold and time intervals as well as table of number of chocolate sold and number of machines. Visualise n-dimensional cube-data; cube means integration of fact table with cross-dimensional tables, visualising the slices and dice faces. When visualising analysed results, first take a whole, then region wise and then individual ACVMs means drilling down views (from coarse granularity of analysis to finer granularity analysis). Next when visualising analysed results, first consider flavour wise and then as a whole means rolling up views (from finer granularity of analysis to coarser granularity analysis). \n\nAdvanced Analytics: Predictive Analytics \n\nPredictive analytics answer the question \u201dWhat will happen?\u201d Predictive analytics is advanced analytics. The user interprets the outputs from advanced analytics using descriptive analytics methods, such as data visualisation. For example, output predictions are visualised along with the yearly sales growth of past five years and predicts next two years sales. Another example, output predictions for the next cycle of automobile sells are visualised along yearly cycles of sales growth and fall in past ten years. Visualising can show the effects to increased competition for a product in years ahead and take decisions, such as need to change product mix and introducing new car models. \n\nPredictive analytics uses algorithms, such as regression analysis, correlation, optimisation, and multivariate statistics, and techniques such as modelling, simulation, machine learning, and neural networks. The software tools make the predictive analytics easy to use and understand. The examples are as follows: \n\n\u25cfPredicting trends \n\n\u25cfUndertaking preventive maintenance from earlier models of equipment and device failure rates \n\nPredictive analytics is \n\nadvanced analytics \n\nin which the user interprets the outputs of \n\ndescriptive analytics \n\n186 Internet of Things: Architecture and Design Principles \n\n\u25cfManaging the campaign with integrated marketing strategy from previous studies of effect of campaigns with respect to media types, regions, targeted age group \n\n\u25cfPredicting by identifying patterns, clusters with similar behaviour \n\n\u25cfPredicting based on anomalous characteristics, anomaly detection The results of predictions need verifications from a domain knowledge, and view from multiple angles. \n\nPrescriptive Analytics \n\nPrescriptive analytics answers not only what is anticipated or what will happen or when it will happen, but also why it will happen based on the input from descriptive analytics and business rules. This final phase, additionally to the prediction also suggests actions for deriving benefits from predictions, and shows the implications of the decision options or the optimal solutions or new resource allocation strategies or risk mitigation strategies. Prescriptive analytics suggest best course of actions in the given state or set of inputs and rules. \n\n5.5.2 Event Analytics \n\nEvents definable options are unique, non-interaction or interaction options for the events. Event analytics use event data for events tracking and event reporting. An event has the following components: \n\n\u25cfCategory\u2013an event of chocolate purchase in ACVM example belongs to one category and event of reaching predefined threshold of sell for specific chocolate flavour which belongs to other category \n\n\u25cfAction\u2013sending message from ACVM on completing predefined sell is the action taken on the event \n\n\u25cfLabel (optional) \n\n\u25cfValue (optional)\u2013on event, messaging the number of chocolate of that flavour sold or remaining. Event analytics generate event reports using event metric, such as event counts for a category of events, events acted upon, event pending action, rate of new events generation in that category. \n\n5.5.3 In-memory Data Processing and Analytics \n\nIn-memory option of row or column formats can be selected in certain databases, for example, Oracle dual format architecture database that enables to run the real-time, ad- hoc, analytic queries on IoTs\u2019 data. \n\nIn-memory and On-store Row Format Option (Few Rows and Many Columns) \n\nConsider the transactions of the type, ATM transactions or sales order transactions. Each row has separate record. For example, separate record for each ACVM or each bank customer or each sales order. The columns have data associated with the record. A row \n\nData Acquiring, Organising, Processing and Analytics 187 \n\nformat enables quick access of all columns for a record. OLTP operations run fast in the row format. There are fewer rows and more columns. For example, updates, inserting new transactions or querying the transactions of specific amount. A row format can be optimized for OLTP operations. The operations access only few rows and need quick access to the columns. \n\nA row format, allowing row data, will be brought into the CPU with a single memory reference. Data for each record is together in-memory and on-store. There is a single copy of the table on storage. Recall Example 5.1 for Internet of ACVMs. For example, required chocolates of each flavour for distinct ACVMs in row format in-memory database enable faster querying. \n\nIn-memory and On-store Column Format Option (Few Columns and More Rows) \n\nConsider analytics of the type, monthly sales of chocolates on the ACVMs, enterprise yearly profits. Analytical workloads access few columns but scan the entire data set. Analytics therefore run faster on column format, more rows and few columns. Fast for processing needs few columns and many rows. They typically require aggregation or fusion or compaction also. A columnar format allows for much faster data retrieval when only a few columns in a table are selected because all the data for a column is kept together in-memory in column format option. A single memory access will load many column values into the CPU. It also lends itself to faster filtering and aggregation, making it the most optimised format for analytics. \n\n5.5.4 Real-time Analytics Management \n\nReal-time analytics management means ensuring faster OLTP as well as OLAP. Real-time analytics works both as direct querying using an OLTP database and in a data warehouse and OLAP on queried results. Queries return fast, databases such as Oracle database provides in-memory row format option large speedups for OLTP applications and in- memory column format option for large speedups for OLAP applications. \n\nExample 5.11 \n\nProblem Give example of dual format in-memory architecture of databases. \n\nSolution \n\nIn-memory option of row or column formats can be selected in certain databases. For example, Oracle dual format architecture databases for in-memory columnar as well as row formats. Oracle\u2019s unique dual format architecture allows data to be stored in both row and column format simultaneously. The Oracle database in-memory option is designed to be completely compatible with and transparent to the existing Oracle applications and is trivial to deploy. This eliminates the trade-offs required in those databases which offer only one format option, format for faster access during OLTP or faster access during analytics. Such databases needs generation of second copy for analytics, thus delaying costs, additional storage costs and synchronisation issues. \n\n188 Internet of Things: Architecture and Design Principles \n\nThe Oracle optimiser is in-memory aware. It has been optimized to automatically run analytic queries using the column format, and OLTP queries using the row format. Oracle\u2019s in-memory columnar technology is a pure in-memory format. The in-memory columnar format does not persist on storage. \n\n5.5.5 Analytics using Big Data in IoT/M2M \n\nBig data means extreme amount of data. Big data also means data of high volume, variety and velocity (3Vs) or one which also includes veracity (4Vs). \n\nVolume means data received from number of sources of data, including data sets with sizes beyond the ability of commonly used software tools to acquire, manage and process data within a tolerable elapsed time. \n\nVariety means structured as well as unstructured data in different formats, variety of data on which no SQL (Structured Query Language) applicable. \n\nVelocity means data received with higher rates due to use of number of sources of data. Veracity means variation in data quality for analytics. The analytics need the trustable data, filtered data after removing\u2014anomalous data, non-standard and not cross referencing data. \n\nBig data also refers simply to the use of predictive analytics or other certain advanced methods which extract the value from data. Big data seldom relates to just a particular size of data set. Extreme amount of data means data with additional information\u2014situational information. For example, information from analysis of data of time period, festival days, holidays, and data of different locations, and contextual information, which means data gathered in specific contexts. \n\n5.5.6 Big Data Analytics \n\nBig data is multistructured data while RDMS maintain more structured data. The open source software Hadoop and MapReduce are from Apache Software. They enable storage and analyse the massive amounts of data. Hadoop File System (HDFS), Mahout, a library of machine learning algorithms and HiveQ, a SQL like scripting language software are used for Big data analytics in the Hadoop ecosystem. MapReduce is a programming model and a core of Hadoop. Large data sets process onto a cluster of nodes using MapReduce. Same node runs the algorithm using the data sets at HDFS and processing is at that node itself. \n\nHadoop is an open-source framework. The framework stores and processes big data. The clusters of computing nodes process that data using simple programming models. Processing takes place in a distributed environment. The framework scales up from single server to thousands of processing machines and servers, each offering environment of local storage and processing. Hadoop accesses data in sequential manner and performs batch processing. A new data set results from input data set that also processes sequentially. \n\nBig data is data of high \n\nvolume, variety and velocity, and may also \n\ninclude veracity \n\nData Acquiring, Organising, Processing and Analytics 189 \n\nHBase is an example of columnar format data storage which enables read or write access in real time for very large tables distributed in Hadoop File System (HDFS). HBase is database for big data. Data access is random access. Therefore, it provides fast look-up from large tables and access latency is small. HBase uses big hash tables. HBase can be considered similar to Google\u2019s BigTable. \n\n5.5.7 Data Analytics Architecture and Stack \n\nAnalytics architecture consists of the following layers: \n\n\u25cfData sources layer \n\n\u25cfData storage and processing layer \n\n\u25cfData access and query processing layer \n\n\u25cfData services, reporting and advanced analytics layer Figure 5.4 shows an overview of a reference model for analytics architecture. Figure 5.4 also shows on the right-hand side the layers in the reference model. \n\nSources Acquiring Data IoT/M2M Data Sources \n\nEnterprise Data Sources \n\nExternal Data Sources \n\nOrganised Data Store Layer \n\nAnalytics Applications \n\nAnalytics Applications Support \n\nTraditional DataStore/Data Warehouse \n\nEvent Stream Processing Complex Event Processing \n\nServices, Reporting, Data Visualisations, OLAP, Advance Analytics (Predictive/Prescriptive Analytics) \n\nData Access, SQL, Query Processing, OLTP, ETL, \n\nR-Descriptive Statistics, In-Memory or On-Store \n\nDatabase Processing, MapReduce and Others \n\nApplications Support Layer \n\nFigure 5.4 Analytics Architecture Reference Model \n\nAnalytical sandbox means analytics tools and analytics environment for predictive analytics on multistructured data. Mesos v0.9 is a resources management platform which enable multiple frameworks sharing of cluster of nodes and which is compatible with open analytics stack [data processing (Hive, Hadoop, HBase, Storm), data management (HDFS)]. \n\n190 Internet of Things: Architecture and Design Principles \n\nBerkeley Data Analytics Stack (BDAS) consists of data processing, data management and resource management layers. \n\nApplications, AMP-Genomics and Carat run at the BDAS. Data processing software component provides in-memory processing which processes the data efficiently across the frameworks. AMP stands for Berkeley\u2019s Algorithms, Machines and Peoples Laboratory. \n\nData processing combines batch, streaming and interactive computations. Resource management software component provides for sharing the infrastructure across the frameworks. \n\nFigure 5.5 shows an overview of BDAS architecture which is a reference model for analytics architecture. Figure 5.5 also shows on right-hand side the file system, library of machine learning algorithms and SQL like scripting language software for the Big data analytics in Hadoop ecosystem. \n\nSources Acquiring Data IoT/M2M Data Sources \n\nEnterprise Data Sources \n\nExternal Data Sources \n\nOrganised Data Store Layer \n\nBusiness Analytics and Intelligence Applications \n\nAnalytics Applications Support \n\nTraditional DataStore/Data Warehouse \n\nEvent Stream Processing Complex Event Processing \n\nServices, Reporting, Data Visualisations, OLAP, Advance Analytics (Predictive/Prescriptive Analytics) \n\nData Access, SQL, Query Processing, OLTP, ETL, \n\nR-Descriptive Statistics, In-Memory or On-Store \n\nDatabase Processing, MapReduce and Others \n\nApplications Support Layer \n\nHDFS (Hadoop File System) for Big Data \n\nMahout Distributed and Scalable Library of Machine Learning Algorithms \n\nHiveQL (SQL like Scripting Language) \n\nFigure 5.5 Berkeley data analytics stack architecture \n\nReconfirm Your Understanding \n\n\u25cfOrganised data in database or data store is used for analytics, new facts and decision taking on those facts. Analytics has three phases before deriving new facts and provide business intelligence\u2014 descriptive, predictive and prescriptive analytics. \n\nData Acquiring, Organising, Processing and Analytics 191 \n\n\u25cfAnalytics uses statistical methods to find new parameters, which add value to the data. Analytics enable building models based on selection of right data. Later the models are tested and used for services and processes. \n\n\u25cfAnalytics architecture consists of the following layers\u2014data sources, data storage and processing, data access and query processing and data services, reporting and advanced analytics layer. \n\n\u25cfDescriptive analytics, statistics, data mining and machine learning are analytics tools. Analytics enable the actions, reporting and generating spreadsheet, data visualisations and KPIs. \n\n\u25cfDescriptive analytics looks at past performance. The performance is evaluated from mining the historical data. \n\n\u25cfOLAP uses following steps\u2014identify the dimensions, each with an attribute and hierarchy, analyse cross tabulations. \n\n\u25cfOLAP runs faster on column format, more rows and few columns. OLTP runs faster on row format: few rows and more columns. Dual in-memory formats provide advantage of faster real-time query processing as well as analytics. \n\n\u25cfOLAP enables viewing of analysed data up to desired granularity, viewing slices and cross-referenced tables, each viewable separately on n-dimensional structure faces using dicing functions. \n\n\u25cfPredictive analytics answer the question, \u201dWhat will happen?\u201d Predictive analytics is advanced analytics. The user interprets the outputs from advanced analytics using descriptive analytics methods, such as data visualisation. \n\n\u25cfPrescriptive analytics answers not only what is anticipated or what will happen or when it will happen, but also why it will happen based on the input from descriptive analytics and business rules. \n\n\u25cfEvent analytics use event data, for events tracking and event reporting. Event analytics generate event reports using event metric (event counts, events acted up on, event pending action, rate of new events generation) in each category of events. \n\n\u25cfAnalytics architecture consists of the following layers: Data Sources, Data Storage and Processing, Data Access and Query Processing, Data Services, Reporting and Advanced Analytics Layers. \n\n\u25cfBerkeley Data Analytics Stack consists of data processing, data management and resource management layers. \n\nSelf-Assessment Exercise \n\n1. List the uses of analytics. 2. Explain spreadsheet-based reports and data visualisation with example of a \n\ndaily sales database for ACVMs. 3. List the advantages of descriptive analytics of ACVMs data. 4. What does OLAP mean? 5. List the usages of slicing and dicing functionalities at automobile service \n\ncentre in Internet of automotive components. 6. How do predictive and prescriptive analytics differ? 7. Why do OLTP operations run faster in row format in-memory database? Why \n\ndo OLAP operations run faster in column format in-memory database? \n\n\u2605 \u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605 \u2605\u2605\u2605 \n\n\u2605\u2605 \u2605\u2605 \n\n\n![Image](/src/assets/generated_images/iot2_p218_i0.png)\n\n192 Internet of Things: Architecture and Design Principles \n\n8. What is big data? 9. How does big data analytics differ from structured RDMS analytics? 10. How are the analytics architecture layers used in automobile service centre \n\nfor Internet of automotive components? 11. Explain Berkeley Data Analytics Stack layer software components. 12. How does analytics lead to business intelligence? \n\n\u2605 \u2605 \u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605\u2605 \n\n5.6  KNOWLEDGE ACQUIRING, MANAGING \n\nAND STORING PROCESSES \n\nData Information Knowledge Wisdom (DIKW)8 forms a pyramid. Information is an enriched set of data values when considered in a context and that can be queried upon. Visualisation of data gives information. Spreadsheet gives information. Analytics gives information. \n\n\u201cOnly 5% streetlights are switched on in night in Internet of Streetlights\u201d is information (Example 1.2). \u201cAll ATMs active\u201d is information (Example 2.3). \u201cAll ACVMs filled with chocolates of all five flavours at the moment\u201d is information (Example 5.1). \u201cAn enterprise is showing consistent growth and profits in successive five years\u201d is information about the functioning of an enterprise. \n\nInformation in the given context is an answer to the query or set of queries. The answer comes from processing the data and querying. For example, a balance sheet data is the enriched set of data values using the analytics. \u201cIs the data in enterprise balance sheet showing consistent growth in preceding five years?\u201d is querying of the balance sheet. The answer is the information obtained from the balance sheet. \n\nKnowledge, according to an English dictionary is sharable information and understanding about a subject or context. Information that All ACVMs filled at most times gives understanding and knowledge that \u201cFill service is prompt in attending to the service requests\u201d. Information about the time series data of sale figures for chocolates gives understanding and knowledge that \u201cACVMs give good sales and profit during festive the days near the gardens.\u201d Knowledge comes from researching gives existing information. An enterprise consistent growth is as a result of installation of new machinery five years back, full production during these years and enterprise sales force performance. \n\nIoT data sources continuously generate data, which the applications or processes acquire, organise and integrates or enriches using analytics. Knowledge discovery tools \n\n8 J. Rowley, \u201cThe wisdom hierarchy: Representations of the DIKW hierarchy\u201d, Journal of Information Science, 33(2), pp. 163-19, 2007 \n\nExplain knowledge discovery, knowledge \n\nmanagement and knowledge- management reference \n\narchitecture \n\nLO 5.5 \n\nData Acquiring, Organising, Processing and Analytics 193 \n\nprovide the knowledge at particular point of time as more and more data is processed and analysed. Knowledge is an important asset of an enterprise. \n\nKnowledge Management \n\nKnowledge management9 (KM) is managing knowledge when the new knowledge is regularly acquired, processed and stored. Knowledge management also provisions for replacing the earlier gathered knowledge and managing the life cycle of stored knowledge. \u2018Fill service is prompt\u2019 is temporal knowledge. It may change at a later date. \u2018Enterprise consistent growth\u2019 is temporal knowledge. It may change in sixth year of operations. \n\nA management tool role is to create, control, use, monitor and delete. A KM tool has processes for discovering, using, sharing, replacing with new, creating and managing the knowledge database and information of the enterprise. \n\nWisdom \n\nSensible and reasonable decisions are made using advanced tools which enable wise decision. Wisdom, according to an English dictionary is \u201cAbility to use the experience and knowledge in order to make sensible and reasonable judgment and decisions\u201d. Judgment from the experience and knowledge that \u201cACVMs chain needs adaptation of loyalty point scheme\u201d is wisdom. Judgment from the knowledge of clients of a particular bank, \u201cOperating a free dispensary will improve heath of the clients\u201d, then they will earn more and consequently bank expects to attract bigger deposits is wisdom. \n\n5.6.1 Knowledge-Management Reference Architecture \n\nFigure 5.6 (a) shows a reference architecture for knowledge management. Figure 5.6(b) shows correspondences with the ITU-T reference model four layers and OSI model layers. \n\nThe lowest layer has sublayers for devices data, streaming data sources which provide input for analytics and knowledge. Databases, Business Support Systems (BSSs), Operational Support Systems (OSSs) data can also be additional inputs. \n\nNext higher layer has data adaptation and enrichment sublayers. Adaptation and enrichment sublayers adapt the data from the lowest layer in appropriate forms, such as database, structured data and unstructured data so that it can be used for analytics and processing. \n\nNext higher layer has processing and analytics sublayers. These sublayers are input to information access tools and knowledge discovery tools. \n\nThe highest layer has knowledge acquiring, managing, storing and knowledge life-cycle management; sublayers for managing, storing and knowledge life-cycle management. Knowledge acquires from the use of information access tools and knowledge discovery tools. \n\n9 https://en.wikipedia.org/wiki/Knowledge_management \n\n194 Internet of Things: Architecture and Design Principles \n\nDevice and Gateway Capabilities Layer \n\nIoT/M2M Devices Data, Streaming Data, Business \n\nSupport and Operational Support Systems, \n\nDatabases Sublayers \n\nServices and Applications Capabilities Layer \n\nGeneric and Specific Support Capabilities) Data Adaptation and Enrichment Sublayers \n\nKnowledge Managing, Storing and Knowledge \n\nLife-Cycle Management and Knowledge Acquiring Sublayer \n\nInformation Access and Knowledge Discovery Tools Sublayer \n\nProcessing and Analytics, Services and \n\nApplication Support Sublayers \n\nApplication and Application Support Layer \n\nPhysical/Data link Layer \n\nApplication Layer \n\nAdaptation Layer \n\nFigure 5.6  (a) A reference architecture for the knowledge management (left-hand side) and \n\n(b) Correspondence in terms of ITU-T reference model and OSI layers for IoT/M2M (middle and right-hand side) \n\nReconfirm Your Understanding \n\n\u25cfData Information Knowledge Wisdom forms a pyramid. \n\n\u25cfInformation is an enriched set of data values when considered in a given context that can be queried upon. Visualisation of data gives information. Spreadsheet gives the information. Analytics gives information. \n\n\u25cfKnowledge gathers from researching up on the information. Knowledge is sharable information and understanding about a subject or context. \n\n\u25cfKnowledge management is managing knowledge when new knowledge is acquired, processed and stored. \n\n\u25cfKnowledge management architecture highest layer is knowledge managing, storing, knowledge life- cycle management, knowledge acquiring, information access and knowledge discovery tools. \n\n\u25cfWisdom is the ability to use the experience and knowledge in order to make sensible and reasonable judgments and decisions. \n\nData Acquiring, Organising, Processing and Analytics 195 \n\nSelf-Assessment Exercise \n\n1. Explain Data Information Knowledge Wisdom pyramid. 2. Show diagrammatically, software components at the layers in reference- \n\narchitecture for knowledge management of services at Internet of automotive components. 3. Why does knowledge management include functions for replacing the earlier \n\ngathered knowledge, usage of knowledge discovery tool periodically and managing the life cycle of stored knowledge? Take example of ACPAMS. 4. Explain knowledge management by example of applications, services and \n\nprocesses for Internet of Automatic Chocolate Vending Machines, Internet of ATMs and Internet of automobile components. \n\n\u2605 \u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605\u2605\u2605 \n\nKey Concepts \n\n\u25cfBig data \n\n\u25cfBusiness intelligence \n\n\u25cfBusiness process \n\n\u25cfCEP \n\n\u25cfData acquiring \n\n\u25cfData generation \n\n\u25cfData source types \n\n\u25cfData storage \n\n\u25cfData store \n\n\u25cfData visualisation \n\n\u25cfDatabase \n\n\u25cfDBMS \n\n\u25cfDescriptive analytics \n\n\u25cfDicing functionalities \n\n\u25cfInformation \n\n\u25cfIn-memory database \n\n\u25cfKnowledge \n\n\u25cfNOSQL \n\n\u25cfOLAP \n\n\u25cfOLTP \n\n\u25cfPredictive analytics \n\n\u25cfQuery \n\n\u25cfQuery processing \n\n\u25cfRDBMS \n\n\u25cfRelational database \n\n\u25cfSpreadsheet \n\n\u25cfSQL \n\n\u25cfTransaction \n\n\u25cfTime-series database \n\n\u25cfWisdom \n\nLearning Outcomes \n\nLO 5.1 \n\n\u25cfData, real-time data, events or event-driven data generate from the active or passive devices and other data sources. \n\n\u25cfA data acquisition application interacts and acquires data from the interactions. Large magnitude of data is acquired from a large number of connected devices, especially from machines in industrial plants or embedded components data from large number of automobiles or health devices in ICUs or wireless sensor networks. \n\n\u25cfData storage systems store the data after validation. Data store can be database, relational database at a server or cloud. Data store can be at data warehouse or Big data at a Cloud. \n\n\u25cfData store may consist of multiple schemas or may consist of data in only one scheme, such as a relational database. Data store at server for short reaction times, optimised performance and high security. \n\n196 Internet of Things: Architecture and Design Principles \n\n\u25cfSpatial storage is storage as spatial database which is optimised to store, enables querying the data objects defined in a geometric space, and which is a database for 2D and 3D objects. \n\nLO 5.2 \n\n\u25cfA database is a collection of data. A relational database is a collection of data into multiple tables which relates to each other through special fields. \n\n\u25cfDatabase Management System is a software system, which contains a set of programs specially designed for creation and management of data stored in a database. \n\n\u25cfDatabase transaction is the execution of a specific set of operations on a database. Transactions can be performed on a database. Relational database transaction is the execution of interrelated instructions using relations. \n\n\u25cfQuery processing means using a process, getting the results of the query made to a database. \n\n\u25cfDistributed database is a collection of logically interrelated, cooperating databases over a computer network. Distributed query processing means query processing operations in distributed databases on the same system or networked systems. \n\n\u25cfSQL is language for data access control, schema creation and modifications. NOSQL stands for or Not Only SQL or No-SQL and no integration with applications that are based on SQL. It is used in cloud data store. \n\n\u25cfTime series data means an array of numbers indexed with time (date-time or a range of date- time). \n\nLO 5.3 \n\n\u25cfA transaction is a collection of operations that form a single logical unit of database. OLTP process begins as soon as data or events generate in real time. Batch transactions processing, stream transactions processing, real-time transaction processing, and complex event processing are process methods for data and events of the data sources and data stores. \n\n\u25cfA business process consists of a series of interrelated structured activities or tasks or processes which follow a logical sequence. \n\n\u25cfA process matrix represents series of operations and activities on a given set of inputs that perform a specific task leading to a decision point in the process. \n\n\u25cfBusiness intelligence is a process that enables a business service to extract new facts and knowledge and then undertake better decisions. \n\n\u25cfDistribution of processes reduces the complexity and communication costs, and enables faster responses and smaller processing load at the central system. \n\n\u25cfComplex application integration integrates heterogeneous application architectures and number of processes. \n\n\u25cfSOA is a software architecture model consisting of services, messages, operations and processes. \n\nLO 5.4 \n\n\u25cfOrganised data is used for analytics, new facts and decision taking on those facts. Analytics has three phases before deriving new facts and provide business intelligence: descriptive, predictive and prescriptive analytics. \n\n\u25cfAnalytics uses statistical methods and finds new parameters which add value to the data. \n\n\u25cfAnalytics architecture consists of the following layers: Data Sources, Data Storage and Processing, Data Access and Query Processing and Data Services, Reporting and Advanced Analytics Layers. \n\nData Acquiring, Organising, Processing and Analytics 197 \n\n\u25cfDescriptive analytics, statistics, data mining and machine learning are analytics tools. Analytics enable actions, reporting and generating spreadsheet, data visualisations and KPIs. \n\n\u25cfOLAP runs faster on column format, i.e. more rows and few columns. \n\n\u25cfPredictive analytics answer the question \u201cWhat will happen?\u201d Predictive analytics is advanced analytics. The user interprets the outputs from advanced analytics using descriptive analytics methods, such as data visualisation. \n\n\u25cfPrescriptive analytics answers not only what is anticipated or what will happen or when it will happen, but also why it will happen based on inputs from descriptive analytics and business rules. \n\n\u25cfEvent analytics use event data for event tracking and event reporting. \n\n\u25cfAnalytics architecture consists of the following layers\u2014Data Sources, Data Storage and Processing, Data Access and Query Processing, Data Services, Reporting and Advanced Analytics Layers. \n\nLO 5.5 \n\n\u25cfData Information Knowledge Wisdom forms a pyramid. \n\n\u25cfKnowledge gathers from researching existing new information. Knowledge is sharable information and understanding about a subject or context. \n\n\u25cfWisdom is the ability to use experience and knowledge in order to make sensible and reasonable judgment and decisions. \n\nExercises \n\nObjective Questions \n\nSelect one correct option out of the four in each question. \n\n1. An IoT application or service software components at the applications and services \n\nlayer are (i) Devices data or event or message generation, (ii) Data acquiring, collection, assembling and storage, (iii) Transactions processing, (iv) IoT applications integration with services (v) Business processes, (vi) Complex Event Processing, (vii) Business intelligence, (viii) Analytics, (xi) Data analytics stack, (ix) Intelligence, (x) Knowledge discovery and (xi) Knowledge management. Which is correct? (a) All except (i) to , (vi) to (xi) (b)  (iv) to (viii) \n\n(c)  All except (x) and (xi) (d)  All except (i) 2. A service means (i) a mechanism which enables the provisioning of access to one \n\nor more capabilities, (ii) has an interface for the service which provides access to capabilities, (iii) initiates on message from another service, (iv) consists of a set of related software components and their functionalities, (v) accesses server database, (v) access to each capability is consistent with the constraints and policies, (vi) has a service description, (vii) can advertise for its capabilities, and (vii) can be used with the complex applications integration. Which is correct? \n\n\u2605 \n\n\u2605\u2605 \n\n198 Internet of Things: Architecture and Design Principles \n\n(a) All except (iii) to (v) and (vii) (b) All except (iii) to (v) and (vii) \n\n(c)  All except (v) (d)  All except (vii) 3. (i) Objects in a data store model using classes which the database define, (ii) data \n\nstore includes data repositories such as database, relational database, flat file, (iii) Data store includes data repositories such as spreadsheet, mail server, web server, directory services and VMware, (iv) Data store may be distributed over multiple nodes, and (v) A data store may consist of multiple schemas or may consist of data in only one scheme. Which is correct? (a)  (ii) to (v) (b)  (ii) and (v) \n\n(c) (i), (ii) and (iv) (d)  All 4. Relation database examples are (i) MySQL, (iii) PostGreSQL, (iv) Oracle database \n\ncreated using PL/SQL, (v) Microsoft SQL server using T-SQL, (vi) HBase, (vii) MongoDB, (viii) CouchDB, (ix) a database in which data organised into multiple tables which relates to each other through special fields, and (x) a database in which data organised into flat file table which enables systematic way for the access. Which is correct? (a) All except (x) (b) All except (vi), (vii), (viii) and (x) \n\n(c) All except (v) (d) (i) and (ix) 5. Server management functions are (i) Monitoring of all critical services with SMS and \n\nemail notifications, (ii) the security of systems and the protection, (iii) maintaining confidentiality and privacy of data, (iv) high degree of security and integrity and effective protection of data, files and databases at the organisation, (v) protection of customer data or enterprise internal documents by attackers which includes spam, (vi) mails, unauthorized uses of the access to the server, viruses, malwares and worms, and (vii) strict documentation and audit of all activities. Which is correct? (a) All except (i), (vi) and (vii) (b) All \n\n(c) (i) to (v) (d)  (ii) to (vii) 6. SQL is a language for (i) data querying, updating, inserting, appending and deleting \n\nthe databases, (ii) data access control, schema creation and modifications, (iii) access to server (iv) querying the file, and (v) service. Which is correct? (a)  (i) and (ii) (b) All except (ii) \n\n(c) All except (iii) (d) (i), (ii) and (iv) 7. Decision on real-time data is fast when query processing in data has low latency due \n\nto (i) massively parallel processing, (ii) relational databases, (iii) time series database, (iv) in-memory database, and (v) columnar database. Which is correct? (a) (iii), (iv) and (v) (b) All except (ii) \n\n\u2605\u2605 \n\n\u2605 \n\n\u2605 \n\n\u2605\u2605 \n\n\u2605 \n\nData Acquiring, Organising, Processing and Analytics 199 \n\n(c) (i) to (iv) (d) (i), (iv) and (v) 8. NOSQL is (i) used in cloud data store, (ii) used in data mining, (iii) means no database \n\nschema, (iv) no integration with applications that are based on SQL, and (v) consists of classes of non-relational data storage systems, flexible data models and multiple schemas. Which is correct? (a) All except (ii) (b) All except (i) \n\n(c) (i), (iv) and (v) (d) All 9. OLTP is used in (i) Internet of Streetlights, (ii) Internet of Automatic Chocolate \n\nVending Machines, (iii) Internet of ATMs, (iv) Internet of Automotive Components for Service Centre Predictive Analytics, (v) Internet of RFIDs, (vi) Complex event processing, and (vii) in case of applications when requirements are availability, speed, concurrency and recoverability in databases for real-time data or events. (a) All (b) (iii), (iv), (vi) and (vii) \n\n(c) (iii) and (vii) (d) (iii) to (vii) 10. A layer in business intelligence and business processes architecture reference model \n\nfor internet of automotive components for service centre consists of the following: (a) Datagram transport layer security layer (b) Data access, SQL, query processing, R-descriptive statistics, predictive analytics \n\nlayer (c) Data integration layer (d) Transactions processing layer 11. SOA (i) models the number of services and interrelationships, (ii) is a software \n\narchitecture model which consists of services, messages, operations and processes, (iii) components distribute over a network or the Internet in a high-level business entity, and (iv) new business applications and applications integration architecture in an enterprise can be developed using a SOA. Which is correct? (a) All except (iii) (b) All \n\n(c) All except (iv) (d) (ii) 12. Descriptive analytics enable (i) actions, (ii) reporting or generating spreadsheets, \n\n(iii) statistical analysis, (iii) visualisations from different perspectives of the analysed results, and (iv) creation of key performance indicators, (v) data visualisation of slices and dices from cross reference tables, (vi) creation of in-memory row-format database, and (vii) later on predictions using predictive analytics. Which is correct? (a) (i) to (iv) (b) All except (ii) and (iii) \n\n(c) All (d) All except (vi) 13. Big data means (i) cloud data, (ii) data received from number of sources of data, \n\n(iii) data sets with sizes beyond the ability of commonly used software tools to acquire, manage and process data within a tolerable elapsed time, (iv) unstructured data in \n\n\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605\u2605 \n\n\u2605 \n\n\u2605 \n\n\u2605\u2605 \n\n200 Internet of Things: Architecture and Design Principles \n\npredefined formats, and (v) data on which NOSQL (Structured Query Language) applicable. (a) All except (iv) (b) (iii) to (vii) \n\n(c) All except (i) and (vi) (d) (iii) to (vii) 14. An Oracle Database has (i) in-memory row format option and (ii) in-memory column \n\nformat option. Which is correct? (a) Real-time analytics need both options (b) Real-time analytics need option (ii) \n\n(c) OLTP needs (ii) option (e) OLAP needs (i) option 15. Analytics architecture and Berkeley Data Analytics Stack Architecture consist of \n\n(i) Data sources layer, (ii) Data storage and processing layer, (iii) Data access and query processing layer, and (iv) Data services, reporting and advanced analytics layer. Which is correct? (a) (i) to (iv) in analytics architecture only (b) All except (i) and (ii) \n\n(c) All except (i) (d) All 16. Highest layer at knowledge management reference architecture consists of \n\n(i) knowledge managing, (ii) knowledge storing, (iii) knowledge life-cycle management, (iv) KNOWLEDGE acquiring, (v) information access and knowledge discovery tools, (vi) analytics (vii) processing and (viii) services and databases. Which is correct? (a) All except (viii) (b) All except (iii) \n\n(c) (i) to (v) (d) All \n\nShort-Answer Questions \n\n1. What do the sensor data, real-time data, periodic intervals data, event data, and event \n\ninitiated data mean? Give an example of each in IoT/M2M applications. [LO 5.1] 2. What are the checks, which validate the data? [LO 5.1] 3. What are the methods for data storage, which can be used for analytics later? \n\n[LO 5.1] 4. How do the database and relation database differ? [LO 5.2] 5. What are the functions which spatial database can perform? [LO 5.2] 6. How do you select a chocolate flavour FL1 sales on festive days from transactions \n\nwith the data store? [LO 5.2] 7. Why do the database transactions follow rules of atomicity, consistency, isolation, \n\nand durability? [LO 5.2] 8. List the performed actions during the event-stream processing and complex-event \n\nprocessing. [LO 5.2] 9. How do mathematical operations (profiles, traces, and curves), queries or database \n\ntransactions on time series implement in a Time Series Database (TSDB)? [LO 5.2] \n\n\u2605 \n\n\u2605\u2605 \n\n\u2605 \n\n\u2605 \n\n\u2605\u2605 \u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605 \u2605\u2605 \n\n\u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605\u2605\u2605 \n\nData Acquiring, Organising, Processing and Analytics 201 \n\n10. What are the data types for which CAP theorem holds true? Answer why CAP \n\ntheorem states that out of three properties (consistency, availability and partitions), two are at least present for a service or process? [LO 5.2] 11. What are the types of business processes? [LO 5.3] 12. What are the software components for descriptive analytics? [LO 5.3] 13. When are streaming transactions and batch transactions performed? [LO 5.3] 14. What are the operations preceding business intelligence? [LO 5.3] 15. List reasons for distributed databases reduced complexity, communication costs, \n\nfaster responses and smaller processing load at the central system. [LO 5.3] 16. What are the different visualisations from the OLAP? [LO 5.4] 17. Why does knowledge require replacement and needs use of knowledge discovery \n\ntools at specified periodic intervals? Take an enterprise which services automobiles using descriptive and prescriptive analytics as an example. [LO 5.5] \n\nReview Questions \n\n1. Describe data generation from IoT/M2M devices. [LO 5.1] 2. List the features of relational time series service. [LO 5.2] 3. What is NOSQL and what are the NOSQL usages? [LO 5.2] 4. Describe different types of transaction processing on databases, streaming data and \n\nevents. [LO 5.3] 5. List the features of distributed business process and distributed business process \n\nsystem (DBPS). [LO 5.3] 6. List OLAP functionalities. [LO 5.4] 7. Describe in-memory row format and column format database features and usages. \n\n[LO 5.4] 8. Explain using an example of Data Information Knowledge Wisdom pyramid from \n\nthe sales data acquiring, organising and analytics. [LO 5.5] \n\nPractice Exercises \n\n1. List data types which communicate from RFIDs in Internet of RFIDs. [LO 5.1] 2. List ways in which sensor nodes configures in Internet of Streetlights. [LO 5.1] 3. List the SQL functionalities. [LO 5.2] 4. Which relational database tables in Example 5.2 for Internet of Automobile \n\nComponents are used in ACPAMS Centre? [LO 5.2] 5. Consider Example 5.1 and list the process matrix and interleaved decision points. 6. List the usages of Internet of Automatic Chocolate Vending Machines after analytics \n\nusing three relational database tables.(Example 5.3) [LO 5.3] 7. List the business intelligence obtained in Internet of ATMs. [LO 5.3] 8. Workout the n-dimensional structure of 2-n cross-referenced tables in Internet of \n\nAutomatic Chocolate Vending Machines. [LO 5.4] 9. Assume n devices each in ICU from ten patients communicate data and events onto \n\nthe Internet. List the series of activities and software tools at each layer required for acquiring, storing and analytics. Draw the analytics reference architecture. [LO 5.5] 10. List the processes for knowledge creation. [LO 5.5] \n\n\u2605\u2605 \n\n\u2605 \u2605\u2605 \u2605\u2605 \u2605\u2605\u2605 \n\n\u2605\u2605 \n\n\u2605 \u2605\u2605\u2605 \n\n\u2605 \u2605\u2605 \u2605\u2605 \u2605\u2605\u2605 \n\n\u2605\u2605 \n\n\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605 \u2605\u2605 \u2605\u2605 \u2605\u2605\u2605 \n\n\u2605\u2605 \u2605\u2605 \n\n\u2605 \u2605\u2605\u2605 \n\n\u2605\u2605\u2605 \n\n\u2605 \n\n",
  "createdAt": "2026-02-17"
}