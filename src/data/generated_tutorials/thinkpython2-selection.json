{
  "title": "selection",
  "slug": "thinkpython2-selection",
  "description": "Module from thinkpython2.pdf: selection",
  "tags": [
    "thinkpython2",
    "Elite"
  ],
  "content": "# selection\n\n\n\nAt this point you have learned about Python\u2019s core data structures, and you have seen \nsome of the algorithms that use them. If you would like to know more about algorithms, \nthis might be a good time to read Chapter B. But you don\u2019t have to read it before you go \non; you can read it whenever you are interested. \n\nThis chapter presents a case study with exercises that let you think about choosing data \nstructures and practice using them. \n\n13.1 \nWord frequency analysis \n\nAs usual, you should at least attempt the exercises before you read my solutions. \nExercise 13.1. Write a program that reads a \ufb01le, breaks each line into words, strips whitespace and \npunctuation from the words, and converts them to lowercase. \n\nHint: The string module provides a string named whitespace , which contains space, tab, new- \nline, etc., and punctuation which contains the punctuation characters. Let\u2019s see if we can make \nPython swear: \n\n>>> import string \n>>> string.punctuation \n'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' \n\nAlso, you might consider using the string methods strip , replace and translate . \nExercise 13.2. Go to Project Gutenberg ( http: // gutenberg. org ) and download your favorite \nout-of-copyright book in plain text format. \n\nModify your program from the previous exercise to read the book you downloaded, skip over the \nheader information at the beginning of the \ufb01le, and process the rest of the words as before. \n\nThen modify the program to count the total number of words in the book, and the number of times \neach word is used. \n\nPrint the number of different words used in the book. Compare different books by different authors, \nwritten in different eras. Which author uses the most extensive vocabulary? \n\n126 \nChapter 13. Case study: data structure selection \n\nExercise 13.3. Modify the program from the previous exercise to print the 20 most frequently used \nwords in the book. \nExercise 13.4. Modify the previous program to read a word list (see Section 9.1) and then print all \nthe words in the book that are not in the word list. How many of them are typos? How many of \nthem are common words that should be in the word list, and how many of them are really obscure? \n\n13.2 \nRandom numbers \n\nGiven the same inputs, most computer programs generate the same outputs every time, \nso they are said to be deterministic . Determinism is usually a good thing, since we expect \nthe same calculation to yield the same result. For some applications, though, we want the \ncomputer to be unpredictable. Games are an obvious example, but there are more. \n\nMaking a program truly nondeterministic turns out to be dif\ufb01cult, but there are ways to \nmake it at least seem nondeterministic. One of them is to use algorithms that generate \npseudorandom numbers. Pseudorandom numbers are not truly random because they are \ngenerated by a deterministic computation, but just by looking at the numbers it is all but \nimpossible to distinguish them from random. \n\nThe random module provides functions that generate pseudorandom numbers (which I \nwill simply call \u201crandom\u201d from here on). \n\nThe function random returns a random \ufb02oat between 0.0 and 1.0 (including 0.0 but not 1.0). \nEach time you call random , you get the next number in a long series. To see a sample, run \nthis loop: \n\nimport random \n\nfor i in range(10): \nx = random.random() \nprint(x) \n\nThe function randint takes parameters low and high and returns an integer between low \nand high (including both). \n\n>>> random.randint(5, 10) \n5 \n>>> random.randint(5, 10) \n9 \n\nTo choose an element from a sequence at random, you can use choice : \n\n>>> t = [1, 2, 3] \n>>> random.choice(t) \n2 \n>>> random.choice(t) \n3 \n\nThe random module also provides functions to generate random values from continuous \ndistributions including Gaussian, exponential, gamma, and a few more. \nExercise 13.5. Write a function named choose_from_hist that takes a histogram as de\ufb01ned in \nSection 11.2 and returns a random value from the histogram, chosen with probability in proportion \nto frequency. For example, for this histogram: \n\n13.3. Word histogram \n127 \n\n>>> t = ['a', 'a', 'b'] \n>>> hist = histogram(t) \n>>> hist \n{'a': 2, 'b': 1} \n\nyour function should return 'a' with probability 2/3 and 'b' with probability 1/3 . \n\n13.3 \nWord histogram \n\nYou should attempt the previous exercises before you go on. \nYou can download my \nsolution from https://thinkpython.com/code/analyze_book1.py . You will also need \nhttps://thinkpython.com/code/emma.txt . \n\nHere is a program that reads a \ufb01le and builds a histogram of the words in the \ufb01le: \n\nimport string \n\ndef process_file(filename): \nhist = dict() \nfp = open(filename) \nfor line in fp: \nprocess_line(line, hist) \nreturn hist \n\ndef process_line(line, hist): \nline = line.replace('-', ' ') \n\nfor word in line.split(): \nword = word.strip(string.punctuation + string.whitespace) \nword = word.lower() \nhist[word] = hist.get(word, 0) + 1 \n\nhist = process_file('emma.txt') \n\nThis program reads emma.txt , which contains the text of Emma by Jane Austen. \n\nprocess_file loops through the lines of the \ufb01le, passing them one at a time to \nprocess_line . The histogram hist is being used as an accumulator. \n\nprocess_line uses the string method replace to replace hyphens with spaces before using \nsplit to break the line into a list of strings. It traverses the list of words and uses strip \nand lower to remove punctuation and convert to lower case. (It is a shorthand to say that \nstrings are \u201cconverted\u201d; remember that strings are immutable, so methods like strip and \nlower return new strings.) \n\nFinally, process_line updates the histogram by creating a new item or incrementing an \nexisting one. \n\nTo count the total number of words in the \ufb01le, we can add up the frequencies in the his- \ntogram: \n\ndef total_words(hist): \nreturn sum(hist.values()) \n\n128 \nChapter 13. Case study: data structure selection \n\nThe number of different words is just the number of items in the dictionary: \n\ndef different_words(hist): \nreturn len(hist) \n\nHere is some code to print the results: \n\nprint('Total number of words:', total_words(hist)) \nprint('Number of different words:', different_words(hist)) \n\nAnd the results: \n\nTotal number of words: 161080 \nNumber of different words: 7214 \n\n13.4 \nMost common words \n\nTo \ufb01nd the most common words, we can make a list of tuples, where each tuple contains a \nword and its frequency, and sort it. \n\nThe following function takes a histogram and returns a list of word-frequency tuples: \n\ndef most_common(hist): \nt = [] \nfor key, value in hist.items(): \nt.append((value, key)) \n\nt.sort(reverse=True) \nreturn t \n\nIn each tuple, the frequency appears \ufb01rst, so the resulting list is sorted by frequency. Here \nis a loop that prints the ten most common words: \n\nt = most_common(hist) \nprint('The most common words are:') \nfor freq, word in t[:10]: \nprint(word, freq, sep='\\t') \n\nI use the keyword argument sep to tell print to use a tab character as a \u201cseparator\u201d, rather \nthan a space, so the second column is lined up. Here are the results from Emma : \n\nThe most common words are: \nto \n5242 \nthe \n5205 \nand \n4897 \nof \n4295 \ni \n3191 \na \n3130 \nit \n2529 \nher \n2483 \nwas \n2400 \nshe \n2364 \n\nThis code can be simpli\ufb01ed using the key parameter of the sort function. If you are curi- \nous, you can read about it at https://wiki.python.org/moin/HowTo/Sorting . \n\n13.5. Optional parameters \n129 \n\n13.5 \nOptional parameters \n\nWe have seen built-in functions and methods that take optional arguments. It is possible \nto write programmer-de\ufb01ned functions with optional arguments, too. For example, here is \na function that prints the most common words in a histogram \n\ndef print_most_common(hist, num=10): \nt = most_common(hist) \nprint('The most common words are:') \nfor freq, word in t[:num]: \nprint(word, freq, sep='\\t') \n\nThe \ufb01rst parameter is required; the second is optional. The default value of num is 10. \n\nIf you only provide one argument: \n\nprint_most_common(hist) \n\nnum gets the default value. If you provide two arguments: \n\nprint_most_common(hist, 20) \n\nnum gets the value of the argument instead. In other words, the optional argument over- \nrides the default value. \n\nIf a function has both required and optional parameters, all the required parameters have \nto come \ufb01rst, followed by the optional ones. \n\n13.6 \nDictionary subtraction \n\nFinding the words from the book that are not in the word list from words.txt is a problem \nyou might recognize as set subtraction; that is, we want to \ufb01nd all the words from one set \n(the words in the book) that are not in the other (the words in the list). \n\nsubtract takes dictionaries d1 and d2 and returns a new dictionary that contains all the \nkeys from d1 that are not in d2 . Since we don\u2019t really care about the values, we set them all \nto None. \n\ndef subtract(d1, d2): \nres = dict() \nfor key in d1: \nif key not in d2: \nres[key] = None \nreturn res \n\nTo \ufb01nd the words in the book that are not in words.txt , we can use process_file to build \na histogram for words.txt , and then subtract: \n\nwords = process_file('words.txt') \ndiff = subtract(hist, words) \n\nprint(\"Words in the book that aren't in the word list:\") \nfor word in diff: \nprint(word, end=' ') \n\nHere are some of the results from Emma : \n\n130 \nChapter 13. Case study: data structure selection \n\nWords in the book that aren't in the word list: \nrencontre jane's blanche woodhouses disingenuousness \nfriend's venice apartment ... \n\nSome of these words are names and possessives. Others, like \u201crencontre\u201d, are no longer in \ncommon use. But a few are common words that should really be in the list! \nExercise 13.6. Python provides a data structure called set that provides many common set \noperations. \nYou can read about them in Section 19.5, or read the documentation at http: \n// docs. python. org/ 3/ library/ stdtypes. html# types-set . \n\nWrite a program that uses set subtraction to \ufb01nd words in the book that are not in the word list. \nSolution: https: // thinkpython. com/ code/ analyze_ book2. py . \n\n13.7 \nRandom words \n\nTo choose a random word from the histogram, the simplest algorithm is to build a list with \nmultiple copies of each word, according to the observed frequency, and then choose from \nthe list: \n\ndef random_word(h): \nt = [] \nfor word, freq in h.items(): \nt.extend([word] * freq) \n\nreturn random.choice(t) \n\nThe expression [word] * freq creates a list with freq copies of the string word . \nThe \nextend method is similar to append except that the argument is a sequence. \n\nThis algorithm works, but it is not very ef\ufb01cient; each time you choose a random word, it \nrebuilds the list, which is as big as the original book. An obvious improvement is to build \nthe list once and then make multiple selections, but the list is still big. \n\nAn alternative is: \n\n1. Use keys to get a list of the words in the book. \n\n2. Build a list that contains the cumulative sum of the word frequencies (see Exer- \ncise 10.2). The last item in this list is the total number of words in the book, n . \n\n3. Choose a random number from 1 to n . Use a bisection search (See Exercise 10.10) to \n\ufb01nd the index where the random number would be inserted in the cumulative sum. \n\n4. Use the index to \ufb01nd the corresponding word in the word list. \n\nExercise 13.7. Write a program that uses this algorithm to choose a random word from the book. \nSolution: https: // thinkpython. com/ code/ analyze_ book3. py . \n\n13.8 \nMarkov analysis \n\nIf you choose words from the book at random, you can get a sense of the vocabulary, but \nyou probably won\u2019t get a sentence: \n\n13.8. Markov analysis \n131 \n\nthis the small regard harriet which knightley's it most things \n\nA series of random words seldom makes sense because there is no relationship between \nsuccessive words. For example, in a real sentence you would expect an article like \u201cthe\u201d to \nbe followed by an adjective or a noun, and probably not a verb or adverb. \n\nOne way to measure these kinds of relationships is Markov analysis, which characterizes, \nfor a given sequence of words, the probability of the words that might come next. For \nexample, the song Eric, the Half a Bee begins: \n\nHalf a bee, philosophically, \nMust, ipso facto, half not be. \nBut half the bee has got to be \nVis a vis, its entity. D\u2019you see? \n\nBut can a bee be said to be \nOr not to be an entire bee \nWhen half the bee is not a bee \nDue to some ancient injury? \n\nIn this text, the phrase \u201chalf the\u201d is always followed by the word \u201cbee\u201d, but the phrase \u201cthe \nbee\u201d might be followed by either \u201chas\u201d or \u201cis\u201d. \n\nThe result of Markov analysis is a mapping from each pre\ufb01x (like \u201chalf the\u201d and \u201cthe bee\u201d) \nto all possible suf\ufb01xes (like \u201chas\u201d and \u201cis\u201d). \n\nGiven this mapping, you can generate a random text by starting with any pre\ufb01x and choos- \ning at random from the possible suf\ufb01xes. Next, you can combine the end of the pre\ufb01x and \nthe new suf\ufb01x to form the next pre\ufb01x, and repeat. \n\nFor example, if you start with the pre\ufb01x \u201cHalf a\u201d, then the next word has to be \u201cbee\u201d, \nbecause the pre\ufb01x only appears once in the text. The next pre\ufb01x is \u201ca bee\u201d, so the next \nsuf\ufb01x might be \u201cphilosophically\u201d, \u201cbe\u201d or \u201cdue\u201d. \n\nIn this example the length of the pre\ufb01x is always two, but you can do Markov analysis with \nany pre\ufb01x length. \nExercise 13.8. Markov analysis: \n\n1. Write a program to read a text from a \ufb01le and perform Markov analysis. The result should be \na dictionary that maps from pre\ufb01xes to a collection of possible suf\ufb01xes. The collection might \nbe a list, tuple, or dictionary; it is up to you to make an appropriate choice. You can test your \nprogram with pre\ufb01x length two, but you should write the program in a way that makes it easy \nto try other lengths. \n\n2. Add a function to the previous program to generate random text based on the Markov analysis. \nHere is an example from Emma with pre\ufb01x length 2: \n\nHe was very clever, be it sweetness or be angry, ashamed or only amused, at such \na stroke. She had never thought of Hannah till you were never meant for me?\" \"I \ncannot make speeches, Emma:\" he soon cut it all himself. \n\nFor this example, I left the punctuation attached to the words. The result is almost syntacti- \ncally correct, but not quite. Semantically, it almost makes sense, but not quite. \n\nWhat happens if you increase the pre\ufb01x length? Does the random text make more sense? \n\n132 \nChapter 13. Case study: data structure selection \n\n3. Once your program is working, you might want to try a mash-up: if you combine text from \ntwo or more books, the random text you generate will blend the vocabulary and phrases from \nthe sources in interesting ways. \n\nCredit: This case study is based on an example from Kernighan and Pike, The Practice of Pro- \ngramming , Addison-Wesley, 1999. \n\nYou should attempt this exercise before you go on; then you can download my so- \nlution from https://thinkpython.com/code/markov.py . You will also need https:// \nthinkpython.com/code/emma.txt . \n\n13.9 \nData structures \n\nUsing Markov analysis to generate random text is fun, but there is also a point to this \nexercise: data structure selection. In your solution to the previous exercises, you had to \nchoose: \n\n\u2022 How to represent the pre\ufb01xes. \n\n\u2022 How to represent the collection of possible suf\ufb01xes. \n\n\u2022 How to represent the mapping from each pre\ufb01x to the collection of possible suf\ufb01xes. \n\nThe last one is easy: a dictionary is the obvious choice for a mapping from keys to corre- \nsponding values. \n\nFor the pre\ufb01xes, the most obvious options are string, list of strings, or tuple of strings. \n\nFor the suf\ufb01xes, one option is a list; another is a histogram (dictionary). \n\nHow should you choose? The \ufb01rst step is to think about the operations you will need to \nimplement for each data structure. For the pre\ufb01xes, we need to be able to remove words \nfrom the beginning and add to the end. For example, if the current pre\ufb01x is \u201cHalf a\u201d, and \nthe next word is \u201cbee\u201d, you need to be able to form the next pre\ufb01x, \u201ca bee\u201d. \n\nYour \ufb01rst choice might be a list, since it is easy to add and remove elements, but we also \nneed to be able to use the pre\ufb01xes as keys in a dictionary, so that rules out lists. With tuples, \nyou can\u2019t append or remove, but you can use the addition operator to form a new tuple: \n\ndef shift(prefix, word): \nreturn prefix[1:] + (word,) \n\nshift takes a tuple of words, prefix , and a string, word , and forms a new tuple that has \nall the words in prefix except the \ufb01rst, and word added to the end. \n\nFor the collection of suf\ufb01xes, the operations we need to perform include adding a new \nsuf\ufb01x (or increasing the frequency of an existing one), and choosing a random suf\ufb01x. \n\nAdding a new suf\ufb01x is equally easy for the list implementation or the histogram. Choosing \na random element from a list is easy; choosing from a histogram is harder to do ef\ufb01ciently \n(see Exercise 13.7). \n\nSo far we have been talking mostly about ease of implementation, but there are other fac- \ntors to consider in choosing data structures. One is run time. Sometimes there is a theoreti- \ncal reason to expect one data structure to be faster than other; for example, I mentioned that \n\n13.10. Debugging \n133 \n\nthe in operator is faster for dictionaries than for lists, at least when the number of elements \nis large. \n\nBut often you don\u2019t know ahead of time which implementation will be faster. One option is \nto implement both of them and see which is better. This approach is called benchmarking . \nA practical alternative is to choose the data structure that is easiest to implement, and then \nsee if it is fast enough for the intended application. If so, there is no need to go on. If not, \nthere are tools, like the profile module, that can identify the places in a program that take \nthe most time. \n\nThe other factor to consider is storage space. For example, using a histogram for the col- \nlection of suf\ufb01xes might take less space because you only have to store each word once, no \nmatter how many times it appears in the text. In some cases, saving space can also make \nyour program run faster, and in the extreme, your program might not run at all if you run \nout of memory. But for many applications, space is a secondary consideration after run \ntime. \n\nOne \ufb01nal thought: in this discussion, I have implied that we should use one data structure \nfor both analysis and generation. But since these are separate phases, it would also be pos- \nsible to use one structure for analysis and then convert to another structure for generation. \nThis would be a net win if the time saved during generation exceeded the time spent in \nconversion. \n\n13.10 \nDebugging \n\nWhen you are debugging a program, and especially if you are working on a hard bug, \nthere are \ufb01ve things to try: \n\nReading: Examine your code, read it back to yourself, and check that it says what you \nmeant to say. \n\nRunning: Experiment by making changes and running different versions. Often if you \ndisplay the right thing at the right place in the program, the problem becomes obvi- \nous, but sometimes you have to build scaffolding. \n\nRuminating: Take some time to think! What kind of error is it: syntax, runtime, or seman- \ntic? What information can you get from the error messages, or from the output of the \nprogram? What kind of error could cause the problem you\u2019re seeing? What did you \nchange last, before the problem appeared? \n\nRubberducking: If you explain the problem to someone else, you sometimes \ufb01nd the \nanswer before you \ufb01nish asking the question. \nOften you don\u2019t need the other \nperson; you could just talk to a rubber duck. \nAnd that\u2019s the origin of the well- \nknown strategy called rubber duck debugging . \nI am not making this up; see \nhttps://en.wikipedia.org/wiki/Rubber_duck_debugging . \n\nRetreating: At some point, the best thing to do is back off, undoing recent changes, until \nyou get back to a program that works and that you understand. Then you can start \nrebuilding. \n\n134 \nChapter 13. Case study: data structure selection \n\nBeginning programmers sometimes get stuck on one of these activities and forget the oth- \ners. Each activity comes with its own failure mode. \n\nFor example, reading your code might help if the problem is a typographical error, but \nnot if the problem is a conceptual misunderstanding. If you don\u2019t understand what your \nprogram does, you can read it 100 times and never see the error, because the error is in \nyour head. \n\nRunning experiments can help, especially if you run small, simple tests. But if you run \nexperiments without thinking or reading your code, you might fall into a pattern I call \n\u201crandom walk programming\u201d, which is the process of making random changes until the \nprogram does the right thing. Needless to say, random walk programming can take a long \ntime. \n\nYou have to take time to think. Debugging is like an experimental science. You should have \nat least one hypothesis about what the problem is. If there are two or more possibilities, try \nto think of a test that would eliminate one of them. \n\nBut even the best debugging techniques will fail if there are too many errors, or if the code \nyou are trying to \ufb01x is too big and complicated. Sometimes the best option is to retreat, \nsimplifying the program until you get to something that works and that you understand. \n\nBeginning programmers are often reluctant to retreat because they can\u2019t stand to delete a \nline of code (even if it\u2019s wrong). If it makes you feel better, copy your program into another \n\ufb01le before you start stripping it down. Then you can copy the pieces back one at a time. \n\nFinding a hard bug requires reading, running, ruminating, and sometimes retreating. If \nyou get stuck on one of these activities, try the others. \n\n13.11 \nGlossary \n\ndeterministic: Pertaining to a program that does the same thing each time it runs, given \nthe same inputs. \n\npseudorandom: Pertaining to a sequence of numbers that appears to be random, but is \ngenerated by a deterministic program. \n\ndefault value: The value given to an optional parameter if no argument is provided. \n\noverride: To replace a default value with an argument. \n\nbenchmarking: The process of choosing between data structures by implementing alter- \nnatives and testing them on a sample of the possible inputs. \n\nrubber duck debugging: Debugging by explaining your problem to an inanimate object \nsuch as a rubber duck. Articulating the problem can help you solve it, even if the \nrubber duck doesn\u2019t know Python. \n\n13.12 \nExercises \n\nExercise 13.9. The \u201crank\u201d of a word is its position in a list of words sorted by frequency: the most \ncommon word has rank 1, the second most common has rank 2, etc. \n\n13.12. Exercises \n135 \n\nZipf\u2019s law describes a relationship between the ranks and frequencies of words in natural languages \n( http: // en. wikipedia. org/ wiki/ Zipf\u2019s_ law ). Speci\ufb01cally, it predicts that the frequency, \nf, of the word with rank r is: \n\nf = cr \u2212 s \n\nwhere s and c are parameters that depend on the language and the text. If you take the logarithm of \nboth sides of this equation, you get: \n\nlog f = log c \u2212 s log r \n\nSo if you plot log f versus log r, you should get a straight line with slope \u2212 s and intercept log c. \n\nWrite a program that reads a text from a \ufb01le, counts word frequencies, and prints one line for each \nword, in descending order of frequency, with log f and log r. Use the graphing program of your \nchoice to plot the results and check whether they form a straight line. Can you estimate the value of \ns? \n\nSolution: https: // thinkpython. com/ code/ zipf. py . To run my solution, you need the plot- \nting module matplotlib . If you installed Anaconda, you already have matplotlib ; otherwise you \nmight have to install it. \n\n136 \nChapter 13. Case study: data structure selection \n\n\n## Chapter 14\n\n\n",
  "createdAt": "2026-02-17"
}